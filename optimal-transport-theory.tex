\newpage
\chapter{Monge \& Kantorovich}

Let's start by providing some definitions that will be used throughout this section.
\begin{definition}
  Given $(\Omega,\mathcal F)$ where $\mathcal F$ is a $\sigma$-algebra,
  then, $\mu: \mathcal F \to [0,+\infty]$ is a measure if:
  \begin{enumerate}[i)]
    \item $\mu(\varnothing)=0$
    \item $(A_n)_{n\in \mathbb N} \subset \mathcal F$ with
          $A_j \cap A_i = \varnothing ,\ \forall i,j \in \mathbb N\implies
            \mu(\cup_{n \in \mathbb N}A_n) = \sum_{n \in \mathbb N}\mu(A_n)$
  \end{enumerate}
  We say that $\mu$ is a probability measure if besides the two
  properties above, we also have $\mu(\Omega) = 1$.
\end{definition}

\begin{definition}
  We call $\mathcal P(X)$ the space of probability measures defined
  on $(X,\mathcal F)$, where the $\sigma$-algebra $\mathcal F$
  is implicit and usually refers to the Borel $\sigma$-algebra.
\end{definition}

\begin{definition}(Pushforward)
  Let $(X,\mathcal F)$ and $(Y, \mathcal G)$ be measurable spaces, $T : X \to Y$ a measurable map
  and $\mu \in \mathcal P(X)$. We call $T_\# \mu$ the
  pushforward of $\mu$, where:
  \begin{equation}
    T_\#\mu(B) = \mu(T^{-1}(B)),\quad \forall B \in \mathcal G
  \end{equation}
\end{definition}

\begin{theorem}
  Let $T: X \to Y$ be a measurable map between
  $(X, \mathcal F, \mu)$ and $(Y, \mathcal G)$. Then,
  $T_\# \mu$ is a measure on $(Y, \mathcal G)$ and
  $\forall f$ measurable and integrable with respect to
  $T_\#\mu$ one has:
  \begin{equation}
    \int_Y f dT_\#\mu = \int_X f \circ T d\mu
  \end{equation}
  \label{thm:pushforward}
\end{theorem}
\begin{prf}
  Let $f_n$ be a simple positive measurable function. Hence
  \begin{equation*}
    \begin{multlined}
      f_n(y) = \sum^N_{i=0} a_i \mathbbm 1_{A_i}(y) \ \therefore
      \int_Y f_n \ dT_\# \mu =
      \sum^N_{i=0} a_i T_\# \mu(A_i) =
      \sum^N_{i=0} a_i \mu(T^{-1}(A_i) )
      \\
      (f_n\circ T)(x) =
      \sum^N_{i=0} a_i \mathbbm 1_{A_i}(T(x))=
      \sum^N_{i=0} a_i \mathbbm 1_{T^{-1}(A_i)}(x)
      \\
      \therefore
      \\
      \int_X f_n \circ T \ d\mu =
      \sum^N_{i=0} a_i \mu(T^{-1}(A_i) ) \\
    \end{multlined}
  \end{equation*}

  Hence, $\int_X f_n \circ T \ d\mu = \int_Y f_n \ dT_\# \mu$.

  Now, for a positive integrable measurable function
  $f$, there exists a sequence
  of positive simple functions such that $f_n \uparrow f$. Then,
  by the Monotone Convergence Theorem,
  \begin{align*}
    \int_Y f \ dT_\# \mu =
    \int_Y\lim_{n\to +\infty}  f_n \ dT_\# \mu & =
    \lim_{n\to +\infty} \int_Y f_n \ dT_\# \mu =                                                 \\
                                               & =\lim_{n\to +\infty}	\int_X f_n \circ T \ d\mu =
    \int_Y f \ dT_\# \mu
  \end{align*}

  If $f$ is non-positive, just use the same argument by splitting
  the negative and positive portions of the function.

\end{prf}

With these definitions, we can enunciate the so called Monge Problem,
which is known as the motivating problem that gave birth to the field
of Optimal Transport.

\begin{definition} (Monge Problem)
  Given two probability measures $\mu \in \mathcal P(X)$,
  $\nu \in \mathcal{P}(Y)$ and a cost function
  $c:X\times Y \to[0,+\infty]$, solve:
  \begin{flalign}
    (MP) &&
    \inf
    \left\{
    \int_{X} c(x,T(x))d\mu \quad : \quad
    T_\# \mu = \nu
    \right\}&&
  \end{flalign}

\end{definition}

In the Monge Problem, no mass can be split. Therefore, one can easily
come up with situations in which there is no solution to the problem,
as shown in \ref{fig:monge_map_example}. A viable solution $T$ to MP
is called a \textbf{Transport Map}.
\begin{figure}[H]
  \centering
  \def\svgscale{0.8}
  \includesvg[inkscapelatex=false]{Figures/monge_map_example.svg}
  \caption{Example of two Optimal Transport Problems. On the left, there exists an optimal transport
    plan, while on the right there is no possible solution.}
  \label{fig:monge_map_example}
\end{figure}

The Monge Problem is hard to solve, and, as we stated, it might not have
a solution. Hence, this problem can be relaxed, becoming the so called
Kantorovich Problem. This relaxation consists of allowing mass to be
split, thus making the set of possible solutions larger.
Before stating the Kantorovich Problem, let's
introduce some more definitions.


\begin{definition}(Projection and Marginal)
  Let $\gamma \in \mathcal P(X\times Y)$ and $\pi_x: X \times Y \to X$
  such that $\pi_x(x,y) = x, \forall (x,y) \in X\times Y$. Hence,
  we say that $\pi_x$ is the projection operator on $X$. We then call
  $(\pi_x)_\#\gamma = \mu$ the marginal distribution of $\gamma$ with
  respect to $X$.

  Equivalently, if for every measurable set $A \subset X$, we have
  $\gamma(A\times Y) = \mu(A)$, then $\mu$ is the marginal of $\gamma$
  with respect to $X$.

  \begin{corollary}
    \label{cor_marginals}
    Given $\gamma \in \mathcal P(X \times Y)$, $\mu$ and $\nu$ are the
    marginals in $X$ and $Y$, respectively $\iff$ For every $f,g$
    integrable measurable non-negative functions, we have
    $$
      \int_{X\times Y} f+g \ d\gamma = \int_X f d\mu + \int_Y g d\nu
    $$
  \end{corollary}
  \begin{prf}
    $\implies$) Note that $(f \circ \pi_x)(x,Y) = f(\pi_x(x,Y))=f(x)$,
    therefore,
    $$
      \int_{X\times Y} f(x) \ d\gamma = \int_{X \times Y} f \circ
      \pi_x(x,y) \ d\gamma \underset{Theo. 1}{=} \int_X f \ d
      (\pi_x)_\# \gamma = \int_X f \ d\mu
    $$

    $\impliedby$) If for all
    integrable measurable non-negative functions $f,g$ we have
    $$
      \int_{X\times Y} f+g \ d\gamma = \int_x f d\mu + \int_Y g d\nu
    $$
    Then, for any $A \subset X$ measurable, make $f(x) = \mathbbm 1_A(x)$
    and $g(y) =0$. Hence,
    $$
      \gamma(A\times Y) =
      \int_{X \times Y} \mathbbm 1_{A \times Y}(x,y) \ d \gamma =
      \int_{X \times Y} \mathbbm 1_A(x) \ d \gamma =
      \int_{X} \mathbbm 1_A(x) \ d \mu = \mu(A)
    $$

  \end{prf}
\end{definition}

\begin{definition} (Coupling)
  Let $(X,\mu)$ and $(Y,\nu)$ be probability spaces. For
  $\gamma \in \mathcal{P}(X\times Y)$, we say that $\gamma$
  is a coupling of $(\mu,\nu)$ if $(\pi_x)_\# \gamma = \mu$
  and $(\pi_y)_\# \gamma = \nu$. Also, we call $\Pi(\mu,\nu)$
  the set of \textbf{Transport Plans}:
  \begin{equation}
    \Pi(\mu,\nu) :=
    \left \{
    \gamma \in \mathcal{P}(X \times Y) \ :
    \ (\pi_x)_\# \gamma = \mu \quad
    \text{and} \quad
    (\pi_y)_\# \gamma = \nu
    \right \}
  \end{equation}
\end{definition}

Finally, we can state the Kantorovich Problem.

\begin{definition} (Kantorovich Problem)
  Given two probability measures $\mu \in \mathcal P(X)$,
  $\nu \in \mathcal{P}(Y)$ and a cost function
  $c:X\times Y \to[0,+\infty]$, solve:
  \begin{flalign}
    (KP) &&
    \inf
    \left\{
    \int_{X \times Y} c(x,y)d\gamma \ : \
    \gamma \in \Pi(\mu,\nu)
    \right\}&&
    \label{eq:KP2}
  \end{flalign}
  \label{def:KP}
\end{definition}

One can prove that indeed every time the Monge Problem has a
solution, so will the Kantorovich Problem. More than that,
the minimal cost of both problems will indeed coincide.
Note that when the Monge Problem has a solution $T:X\to Y$, then
$\gamma	= (id,T)_\# \mu$ is a solution to the Kantorovich Problem.

We stated in the beginning of this section that (KP) was a relaxed
version of (MP). Let's now formalize this concept.

\begin{definition}(Lower Semi-Continuity)
  A function $f:X \to \mathbb R$ is lower semi-continuous (l.s.c) if
  \begin{equation}
    \forall x \in X, \ f(x) \leq
    \underset{n\to +\infty}{\liminf}f(x_n)
  \end{equation}
  \label{def:lsc}
\end{definition}

\begin{definition}(Relaxation)
  Given a metric space X and
  functional $F:X \to\mathbb R \cup \{+\infty\}$ bounded below. We
  call $\bar F : X \to \mathbb R \cup \{+\infty\}$ a of relaxation
  of $F$ if:
  \begin{equation}
    \bar F(x) := \inf \left \{
    \liminf_n F(x_n) \ : \ x_n \to x
    \right\}
  \end{equation}
  Hence, $\bar F$ is the maximal functional $G$ where $G$ is
  lower semi-continuous and $G \leq F$.
\end{definition}

Below in Figure \ref{fig:relaxation_ex}
we present an example of a relaxation with the aim of improving
the intuition regarding the definition. Note that, as a
consequence of this definition, $\inf_x F = \inf_x \bar F$. Therefore,
if we can prove that Kantorovich Problem is a relaxation of
the Monge  Problem, we would get that
$\inf \text{(KP)} = \inf \text{(MP)}$


\begin{figure}[H]
  \centering
  \includesvg[inkscapelatex=false]{Figures/relaxation_example.svg}
  \caption{Example of a function F and it's relaxation.}
  \label{fig:relaxation_ex}
\end{figure}

To prove that indeed (KP) is a relaxation of (MP) under some conditions,
we use the following theorem, for which the complete proof can be found
on \citet{santambrogio2015optimal}.

\begin{theorem}(Santambrogio 1.32)
  Let $\Omega \subset \mathbb R^d$ compact, with
  $c:\Omega\times \Omega: \to [0,+\infty]$ continuous and
  $\mu \in \mathcal P(\Omega)$ atomless (i.e., for every
  $x \in \Omega$, we have $\mu(\{x\}) = 0)$.
  Then, the set of plans
  $\gamma_T = (id, T)_\# \mu$ induced by the map $T$ is dense in
  $\Pi(\mu,\nu)$.
  \label{thm:dense_mp}
\end{theorem}

We can now prove the following:

\begin{theorem}
  For $\Omega \subset \mathbb R^d$ compact,
  $c:\Omega\times \Omega: \to [0,+\infty]$ continuous and
  $\mu \in \mathcal P(\Omega)$ atomless. Then, (KP) is a relaxation
  of (MP).
\end{theorem}
\begin{prf}
  First, let's restate the Monge Problem as
  \begin{equation*}
    \inf \{J(\gamma) \ : \ \gamma \in \Pi(\mu,\nu)\}
  \end{equation*}
  Where
  \begin{equation*}
    J(\gamma)  =
    \begin{cases}
      K(\gamma)=
      \int_{\Omega} c(x,T(x)) \ d\mu =
      \int_{\Omega \times \Omega}c \ d\gamma_T,
              & \text{if } \gamma = \gamma_T \\
      +\infty & \text{otherwise}
    \end{cases}
  \end{equation*}

  Note that indeed minimizing $J$ is equal to minimizing the
  Monge Problem, since we only consider the transport plans
  $\gamma_T$ that coincide with the cost when using a transport map
  $T$.

  For $K(\gamma) = \int_{\Omega \times \Omega} c \ d\gamma$,
  we can show that $K$ is continuous with respect to weak convergence (see \ref{def:weakconv}), since
  \begin{align*}
    \gamma_n \rightharpoonup \gamma \iff
    \forall f \text{ continuous}, \int f d\gamma_n \to \int f d\gamma
    \implies
    \\
    \implies
    K(\gamma_n) = \int_{\Omega \times \Omega} c \ d\gamma_n \to
    K(\gamma)\text{, for } c \text{ continuous.}
  \end{align*}

  Also, by the definition of $J$, for any $\gamma \in \Pi(\mu,\nu)$, then $K(\gamma) \leq J(\gamma)$.

  By Theorem \ref{thm:dense_mp}, for any
  $\gamma \in \Pi(\mu,\nu)$ we can create a sequence of
  $\gamma_{T_n}\rightharpoonup \gamma$. And by the continuity
  of $K$ with respect to weak convergence, we have that $J(\gamma_{T_n})=K(\gamma_{T_n})\to
    K(\gamma)$. Therefore:
  \begin{equation*}
    \forall \gamma \in \Pi(\mu,\nu), \exists (\gamma_{T_n})\ : \
    \liminf_{n\to +\infty} J(\gamma_{T_n})= K(\gamma)
  \end{equation*}
  Hence,
  \begin{equation*}
    \inf\{
    \liminf_{n\to +\infty} J(\gamma_{n}) \ :
    \ \gamma_n \to \gamma
    \}\leq K(\gamma) \leq J(\gamma)
  \end{equation*}

  We can conclude that
  \begin{equation*}
    \inf\{
    \liminf_{n\to +\infty} J(\gamma_{n}) \ :
    \ \gamma_n \to \gamma
    \} = K(\gamma)
  \end{equation*}

\end{prf}

\newpage
\chapter{On the Existence of Transport Plans}
As stated before, it is not trivial to know when the Monge Problem
indeed has a solution. It is easier to work with the Kantorovich
Problem. In this section we present some results that relate
to the existence of Optimal Transport Plans for the Kantorovich Problem.

\begin{theorem}(Santambrogio 1.4)
  Let $X$ and $Y$ be compact metric spaces.
  Given $\mu\in \mathcal{P}(X)$, $\nu \in \mathcal P(Y)$ and
  $c:X\times Y \to[0,+\infty]$, if $c$ is continuous, then
  (KP) admits a solution.
  \label{thm:Santambrogio1.4}
\end{theorem}
\begin{prf}
  We begin by using the notion of weak convergence to characterize
  continuity of functions defined on probability measures.

  Note that since $c$ is continuous and $(X \times Y)$ is compact,
  then $c$ is continuous and bounded. Also,
  $K(\gamma) = \int_{X\times Y}c \ d\gamma$ is continuous with respect to weak
  convergence, since
  $\gamma_n \rightharpoonup \gamma$, if, and only if, for every $f$ continuous
  and bounded function, it's true that $\int f \ d\gamma_n \to \int f \ d\gamma$.

  Now, let's \textbf{show that $\Pi(\mu,\nu)$ is compact}.
  Take $\gamma_n \in \Pi(\mu,\nu)$. Note that $\gamma_n$ is tight (\ref{def:tight}),
  because $(X\times Y)$ is compact. Then, by Prokhorov Theorem \ref{Prokhorov},
  $\exists \gamma_{n_k} \rightharpoonup \gamma$.

  Take $\phi(x) \in C(X)$ and $\psi(y) \in C(Y)$. Therefore,
  \begin{equation*}
    \begin{split}
      \int \phi(x) \ d\mu
      \underset{Cor.\ref{cor_marginals}}{=}
      \int\phi(x)\ d\gamma_{n_k}
      \to
      \int \phi(x) \ d\gamma \\
      \int \psi(y) \ d\nu
      \underset{Cor.\ref{cor_marginals}}{=}
      \int\psi(y)\ d\gamma_{n_k}
      \to
      \int \psi(y) \ d\gamma
    \end{split}
  \end{equation*}
  % \ref{cor_marginals}

  We conclude that $\gamma \in \Pi(\mu,\nu)$, which implies that
  $\Pi(\mu,\nu)$ is compact. Finally, since $K(\cdot)$ is continuous with respect to weak convergence
  and defined on a compact set, it attains a minimum. In other words,
  there exists a transport plan $\gamma$ that minimizes the Kantorovich
  Problem.
\end{prf}

Before going into the next theorem, let's prove a small result.
\begin{lemma}
  Let $(X,d)$ be a metric space and $f_k: X \to \mathbb R$ be l.s.c and bounded from below for every $k \in \mathbb N$.
  Then, $f = \sup_k f_k$ is also l.s.c and bounded from below.
\end{lemma}

\begin{prf}
  Since $f_k > L$, then $\sup_k f_k > L$, thus $f$ is bounded from below.

  Next, since $f_k$ is l.s.c, therefore for $x_n \to x$:
  \begin{equation*}
  f_k(x) \leq \lim_{j} \inf_{n \geq j} f_k(x_n) \implies 
  \sup_k f_k(x) \leq \sup_k \lim_{j} \inf_{n \geq j} f_k(x_n)
  \end{equation*}
  Note that $\inf_{n\geq j} f_k(x_n) \leq \sup_k \inf_{n\geq j}f_k(x_n)$, hence
  \begin{equation*}
    \lim_j \inf_{n\geq j} f_k(x_n) \leq \lim_j \sup_k \inf_{n\geq j}f_k(x_n) \implies
    \sup_k \lim_j \inf_{n\geq j} f_k(x_n) \leq \lim_j \sup_k \inf_{n\geq j}f_k(x_n)
  \end{equation*}
  Also, note that $\inf_{n\geq j} f_k(x_n) \leq \inf_{n\geq j} \sup_k f_k(x_n)$, hence
  \begin{equation*}
    \sup_k \inf_{n \geq j} f_k(x_n) \leq
    \inf_{n \geq j} \sup_k f_k(x_n) \implies 
    \lim_j \sup_k \inf_{n \geq j} f_k(x_n) \leq
    \lim_j \inf_{n \geq j} \sup_k f_k(x_n)
  \end{equation*}
  We conclude that $\sup_k f(x) \leq \lim_j \inf_{n \geq j} \sup_k f_k(x_n)$. So $f$ is l.s.c.

\end{prf}

\begin{theorem}(Santambrogio 1.5)
  \label{teo1.5}
  Let $X$ and $Y$ be compact metric spaces.
  Given $\mu\in \mathcal{P}(X)$, $\nu \in \mathcal P(Y)$ and
  $c:X\times Y \to[0,+\infty]$, if $c$ is lower semi-continuous
  bounded from below, then
  (KP) admits a solution.
\end{theorem}
\begin{prf}

  This proof follows the same ideas from the proof of Theorem \ref{thm:Santambrogio1.4}.
  The only thing we need to prove is that $K(\gamma)$ is l.s.c with respect to weak convergence.

  Let's use that for $c:X\to \mathbb R \cup \{+\infty\}$ bounded from below,
  then, $c$ is l.s.c if and only if there exists a sequence of
  $k-$Lipschitz  functions
  $c_k$ such that
  $\forall x \in X$, $\sup_k c_k(x) = c(x)$.

  Since $c$ is indeed l.s.c and bounded from below, then we know that $c = \sup_k c_k$, and by the
  Monotone Convergence Theorem,
  \begin{equation*}
    K(\gamma) =\int c \ d\gamma =
    \int \sup_k c_k \ d\gamma = \sup_k\int c_k \ d\gamma
  \end{equation*}

  Note that we also know that $c_k$ are Lipschitz, hence, they are also all continuous and bounded.
  This implies that $K_k(\gamma) = \int c_k \ d\gamma$ is also bounded and continuous with respect to weak convergence.
  Therefore, $K(\gamma) = \sup_k K_k(\gamma)$, which implies that $K(\gamma)$ is l.s.c and bounded.
  By the Weierstrass's Theorem, we conclude that
  there exists a transport plan $\gamma$ that minimizes the Kantorovich
  Problem.
\end{prf}

\begin{theorem}(Santambrogio 1.7)
  Let $X$ and $Y$ be Polish (complete and separable) metric spaces.
  Given $\mu\in \mathcal{P}(X)$, $\nu \in \mathcal P(Y)$ and
  $c:X\times Y \to[0,+\infty]$, if $c$ is lower semi-continuous then
  (KP) admits a solution.
  \label{thm:existanceKPpolish}
\end{theorem}
\begin{prf}

  Let's prove that $\Pi(\mu,\nu)$ is compact. To do this,
  we prove that $\Pi(\mu,\nu)$
  is tight (\ref{def:tight}), and therefore, by Prokhorov's Theorem (i) \ref{Prokhorov},
  it is pre-compact. Once this is done,
  the proof follows in the same manner as Theorem
  \ref{thm:Santambrogio1.4}.

  Note that since $\mu$ and $\nu$ are probability measures, then,
  the families $\{\mu\}$ and $\{\nu\}$ each containing only one element
  are pre-compact (actually, compact). Since $X$ is Polish, we can use Prokhorov (ii)
  \ref{Prokhorov}, to conclude that $\mu$ and $\nu$ are tight.
  Hence,
  for $\epsilon > 0, \exists K_X \subset X$ and $K_Y \subset Y$
  both compacts, such that
  $\mu(X\setminus K_X), \nu(Y\setminus K_Y)<\epsilon /2$.

  Next, note that
  \begin{equation*}
    (X \times Y) \setminus (K_X \times K_Y) \subset
    (X \setminus K_X \times Y)\cup
    (X \times Y \setminus K_Y)
  \end{equation*}
  Therefore, for any $\gamma_n \in \Pi(\nu,\mu)$ we obtain
  \begin{equation*}
    \gamma_n((X \times Y) \setminus (K_X \times K_Y)) \leq
    \gamma_n((X \setminus K_X) \times Y) +
    \gamma_n(X \times (Y \setminus K_Y))
  \end{equation*}

  Finally, note that $\gamma_n(A \times Y) = \mu(A)$. Hence,
  \begin{equation*}
    \gamma_n((X \times Y) \setminus (K_X \times K_Y)) \leq
    \mu(X \setminus K_X) +
    \nu(Y \setminus K_Y) < \epsilon
  \end{equation*}

  Which shows that every sequence $\gamma_n \in \Pi(\mu,\nu)$ is
  tight, concluding our proof.


\end{prf}

\newpage
\chapter{Duality of the Kantorovich Problem}

In this chapter we deal with Duality Theorems regarding the Kantorovich Problem.
Under some conditions, the original Kantorovich Problem (Primal) is equivalent to a Dual
formulation, where instead of minimizing transport plans, one seeks to maximize potentials.
Hence, we'll begin this section by introducing the notion of the Dual Problem, and then we'll prove the
equivalence between the Dual and the Primal, starting from more restrictive conditions (e.g. compact spaces)
and moving to more general conditions (e.g. Polish spaces). We finish the section with the celebrated
Kantorovich-Rubinstein's Duality Theorem.

Before introducing the Dual Problem, we need the following result:
\begin{lemma}

  The Kantorovich Problem (\ref{def:KP}) is equivalent to:
\begin{align}
  \inf_{\gamma \in \mathcal M_+(X\times Y)}
  \int_{X \times Y} c(x,y)d\gamma &+
  \sup_{(\phi,\psi) \in B}
  \int_X \phi(x) \ d\mu
  \nonumber
  \\
  &+ \int_Y \psi(y) \ d\nu
  - \int_{X\times Y} \phi(x) + \psi(y) \ d\gamma
  \label{eq:KP2}
\end{align}
Where $B := \{\phi \in C_b(X) \ \mathrm{and} \ \psi \in C_b(Y)\}$.
\end{lemma}
\begin{prf}
  Let's suppose that $\gamma \notin \Pi(\mu,\nu)$.
  Then, without lost of generality, $\exists A \ : \ \mu(A) \neq
    \gamma(A,Y)$. Hence, can make $\phi(x) = M$ in $A$ and null elsewhere.
  So,
  \begin{equation*}
    \int_A \phi \ d\mu - \int_A \phi \ d\gamma	= M(\mu(A)-\gamma(A,Y))
  \end{equation*}
  Since we can make $M$ arbitrarily large or small, we conclude that
  \begin{equation*}
    \sup_{(\phi,\psi) \in B}
    \int_X \phi(x) \ d\mu + \int_Y \psi(y) \ d\nu -
    \int_{X\times Y} \phi(x) + \psi(y) \ d\gamma = +\infty
  \end{equation*}

  This implies that for $\gamma \notin \Pi(\mu,\nu)$, equation \eqref{eq:KP2}
  is $+\infty$. If $\gamma \in \Pi(\mu,\nu)$, then we return to
  \begin{equation*}
    \inf_{\gamma \in \Pi(\mu,\nu)} \int_{X\times Y} c \ d\gamma
  \end{equation*}
  With this, we proved that the argument that minimizes
  equation \eqref{eq:KP2} must
  be inside $\{\gamma \in \Pi(\mu,\nu)\}$, which is the original Kantorovich
  Problem.
\end{prf}

\vspace{5mm}

With (KP) reformulated, the Dual Problem consists of exchanging the order
of the $\inf$ and the $\sup$:
\begin{itemize}
  \item \textbf{Primal}
        \footnote{$(\phi\oplus \psi) (x,y) = \phi(x) + \psi(y)$}:
        \begin{equation}
          \inf_{\gamma \in \mathcal M_+(X\times Y)}
          \sup_{(\phi,\psi) \in B}
          \int_{X \times Y} c \ d\gamma +
          \int_X \phi \ d\mu + \int_Y \psi \ d\nu -
          \int_{X\times Y} \phi \oplus \psi \ d\gamma
        \end{equation}

  \item \textbf{Dual}:
        \begin{equation}
          \sup_{(\phi,\psi) \in B}
          \inf_{\gamma \in \mathcal M_+(X\times Y)}
          \int_{X \times Y} c \ d\gamma +
          \int_X \phi \ d\mu + \int_Y \psi \ d\nu -
          \int_{X\times Y} \phi \oplus \psi \ d\gamma
        \end{equation}
\end{itemize}

Note that in the Dual formulation, we can rewrite it as:
\begin{equation}
  \sup_{(\phi,\psi)\in B}
  \int_X \phi \ d\mu + \int_Y \psi \ d\nu -
  \inf_{\gamma \in \mathcal M_+(X\times Y)}
  \int_{X\times Y} c - (\phi \oplus \psi) \ d\gamma
\end{equation}

If there exists an $A$ such that for all $\forall (x,y) \in A, \ \phi(x) + \psi(y) \geq c(x,y)$, then
$\inf_\gamma \int c - (\phi \oplus \psi) \ d\gamma = -\infty$
since we can choose any $\gamma \in \mathcal M_+(X\times Y)$.

Therefore, we can formally state the Dual Problem as:
\begin{definition}
  Given $\mu \in \mathcal P(X)$, $\nu \in \mathcal P (Y)$ and
  a cost $c:X \times Y \to \mathbb R_+$. The
  Dual Problem is given by
\end{definition}
\begin{flalign}
  \mathrm{(DP)} &&
  \sup \left \{
  \int_X \phi \ d\mu + \int_Y \psi \ d\nu \ :
  \phi \in C_b(X) \ , \psi \in C_b(y) \ ,
  \ \phi \oplus \psi \leq c
  \right \}
  &&
  \label{eqt:dualproblem}
\end{flalign}

We call \textbf{Weak Duality} if
$\mathrm{(DP)} \leq \mathrm{(KP)}$, and we call \textbf{Strong Duality}
if
$\mathrm{(DP)} = \mathrm{(KP)}$.
One can easily prove that for (KP), the Weak Duality is always true.
The more interesting question is ``When does one have Strong Duality?''.

\begin{lemma}
  The Dual Problem for the Kantorovich Problem always satisfies the
  Weak Duality, i.e.
$\mathrm{(DP)} \leq \mathrm{(KP)}$.
\end{lemma}

\begin{prf}
  Since $\phi \oplus \psi \leq c$. Therefore,
  \begin{equation*}
    \int_X \phi \ d\mu +
    \int_Y \psi \ d\nu
    =
    \int_{X \times Y} \phi \oplus \psi \ d\gamma \leq
    \int_{X\times Y} c(x,y) \ d\gamma
  \end{equation*}
\end{prf}

Before starting the proof of duality, we must introduce the concepts
of $c$-transform and $c$-Cyclical monotonicity.

\begin{definition}(c-Transform)
  Given $f: X \to \overline{\mathbb R}$, and
  $c:X\times Y \to \overline{\mathbb R}$,
  the $c$-transform of $f$ is:
  \begin{equation}
    f^c(y) := \inf_x c(x,y) - f(x)
  \end{equation}
  Function $f^c$ is also called the $c$-conjugate of $f$. Moreover,
  we say that $f$ is $c$-concave if
  $\exists \ g:Y\to \overline{\mathbb R}$
  such that $g^c(x) = f(x)$.
  \label{def:c-transform}

  Note that the $c$-transform is a generalization of the
  Legendre-Fenchel transform, which is defined as:
  \begin{equation}
    f^*(y) := \sup_x x \cdot y - f(x)
  \end{equation}
\end{definition}

\begin{lemma}
  Let $c: X \times Y \to \overline{\mathbb R}$ be uniformly continuous. Define two functions
  $\phi:X \to \mathbb R$ and $\psi : Y \to \mathbb R$
  Therefore, $\phi^c$ and $\psi^c$ have the same modulus of continuity
  \footnote{Check Theorem \ref{thm:mod_continuity} for the definition of modulus of continuity}
  as $c$.
  \label{lem:cunif}
\end{lemma}
\begin{prf}
  By Theorem \ref{thm:mod_continuity}, there exists a modulus
  of continuity $\omega$, such that
  \begin{equation*}
    |c(x,y) - c(x',y')| \leq \omega(d(x,x')+d(y,y'))
  \end{equation*}
  Observe that for $g_x(y) = c(x,y) - \phi(x)$
  \begin{equation*}
    |g_x(y) - g_x(y')|=|c(x,y) - c(x,y')| \leq
    \omega(d(x,x)+d(y,y')) = \omega(d(y,y'))
  \end{equation*}
  Hence, $g_x$ has modulus of continuity $\omega$. Now, using the
  Inf-Sup Inequality \ref{lem:infsup_ineq}
  \begin{align*}
    |\inf_x g_x(y) - \inf_x g_x(y')| & =
    |\phi^c(y) - \phi^c(y')| \leq \sup_x |g_x(y) - g_x(y')| =                          \\
                                     & =\sup_x |c(x,y) - c(x,y')| \leq \omega(d(y,y'))
  \end{align*}

  Using the same argument for $\psi^c$, we showed that both
  $c-$transforms have the same modulus of continuity.

\end{prf}

With the definition of $c$-transforms and the lemma above, we can prove the following theorem:

\begin{theorem}(Santambrogio 1.11)

  For $X$ and $Y$ compact metric spaces, and $c:X \times Y \to
    \overline{\mathbb R}$ continuous. Then, the Dual Problem
  has a solution $(\phi,\phi^c)$ for $\phi$ $c-$concave. Hence
  \begin{equation}
    \max(\mathrm{DP}) =
    \max_{\phi \in c-conc.(X)} \int_X \phi \ d\mu +
    \int_Y \phi^c \ d\nu
  \end{equation}
  \label{thm:c-conc}
\end{theorem}
\begin{prf}
  Let $(\phi_n,\psi_n)$ be a maximizing sequence of the Dual problem.
  Note that the $c$-transforms always improve the Dual Problem, since
  $\phi_n\oplus \psi_n \leq c$, which implies that
  \begin{align*}
    \phi_n^c(y):= \inf_x c(x,y) - \phi_n(x) \geq \psi_n(y) \\
    \psi_n^c(x):= \inf_y c(x,y) - \psi_n(y) \geq \phi_n(x) \\
    \int_X \phi_n \ d\mu +
    \int_Y \psi_n \ d\nu \leq
    \int_X \phi_n \ d\mu +
    \int_Y \phi_n^c \ d\nu
  \end{align*}
  Hence, the sequence $(\phi_n, \phi^c_n)$ is also maximizing.

  Since $X \times Y$ is compact, the cost $c$ is uniformly continuous. Therefore,
  by Lemma \ref{lem:cunif}, the $c-$transforms of $\phi_n$ and $\psi_n$ are bounded by the
  same modulus of continuity $\omega$ as the cost function $c$.

  Instead of using
  \begin{equation*}
    \psi^c_n (x) = \inf_y c(x,y) - \psi(y)
  \end{equation*}
  We will use
  \begin{equation*}
    \psi^c_n (x) := \inf_y c(x,y) - \phi_n^c(y) = \phi_n^{c c}(x)
  \end{equation*}
  This sequence is still maximizing, since
  \begin{align*}
    \phi_n^c(y)  = \inf_x c(x,y) - \phi_n(x) \geq \psi_n(y) &\implies 
    \phi_n(x) + \phi_n^c(y) \leq c(x,y)                        \\
    &\implies
    \psi_n^c(x)  = \inf_y c(x,y) - \phi_n^c(y) \geq \phi_n(x)
  \end{align*}

  Therefore, for a maximizing sequence $(\phi_n,\psi_n)$, we can
  instead take the maximizing sequence
  $(\psi^c_n,\phi^c_n)=(\phi^{c c}_n,\phi^c_n)$.

  Our goal now is to use the Àrzela-Ascoli Theorem (\ref{thm:arzela-ascoli}), so
  we can take a subsequence converging uniformly. To use the theorem, we'll
  show that our sequence
  $(\psi^c_n,\phi^c_n)$ is Equicontinuous (see Definition \ref{def:equicontinuous})
  and Equibounded (see definition \ref{def:equibounded}).

  First, note that $(\psi^{c}_n,\phi^c_n)$
  is in fact Equicontinuous, since
  for any $\epsilon > 0$, we can take $\delta >0$ such that
  $d(y,y') < \delta \implies w(d(y,y')) < \epsilon$ and
  $|\phi_n^c(y) - \phi_n^c(y')| \leq w(d(y,y')) < \epsilon$, for every
  $n \in \mathbb N$.
  
  Next, let's prove that the sequence is Equibounded. Taking the supremum of the inequality, we obtain
  \begin{equation*}
    \sup_{y,y'} |\phi^c_n(y) - \phi^c_n(y')| \leq
    \sup_{y,y'}w(d(y,y')) = w(\text{diam}(Y))
  \end{equation*}
  The equality in the equation above is true because the function
  $\omega$ is increasing, and the set $Y$ is compact. Again, the
  same argument works for $\psi_n^c$.

  Next, realize that we can add and subtract constants from
  the Dual Problem without modifying the results:

  \begin{equation*}
    \int_X \psi_n^c \ d\mu + \int_Y \phi_n^c \ d\nu =
    \int_X \psi_n^c + C_n \ d\mu + \int_Y \phi_n^c - C_n \ d\nu
  \end{equation*}

  Let's take $C_n = \min_y \phi_n^c(y)$. We now change the sequence
  of functions to $(\psi_n^c + C_n, \phi_n^c - C_n)$, which preserves
  the maximizing property. Note that $\min_y \phi_n^c - C_n = 0$.
  Hence,

  \begin{align*}
    \sup_{y,y'} |\phi^c_n(y) - \phi^c_n(y')| =
    \max_y \phi_n^c(y) - \min_y \phi_n^c(y) =
    \max_y \phi_n^c(y) \leq \omega(\text{diam}(Y))
  \end{align*}
  Also, for any $x \in X$:
  \begin{align*}
    \psi_n^c(x) = \inf_y c(x,y) - \phi^c_n(y) \in
    [\min_y  \ c(x,y) - \omega(\text{diam}(Y)),\max_y  \ c(x,y) \ ]
  \end{align*}

  With this, we showed that the sequence is Equibounded. Therefore,
  since we are on a compact set and the sequence
  $(\psi_n^c, \phi_n^c)$ is both Equicontinuous and Equibounded,
  we can apply the Àrzela-Ascoli Theorem \ref{thm:arzela-ascoli}.
  Thus, we can obtain a subsequence
  $(\psi_{n_k}^c,\phi_{n_k}^c)$ that converges uniformly to
  $(\psi,\phi)$. As a consequence of this uniform convergence

  \begin{equation*}
    \int_X \psi_{n_k}^c \ d\mu +
    \int_Y \phi_{n_k}^c \ d\nu
    \to
    \int_X \phi \ d\mu +
    \int_Y \psi \ d\nu
  \end{equation*}
  With this, we proved that there exists a pair of functions
  $(\phi, \psi)$ that are the limits of a maximizing sequence and that
  satisfy the constraint (i.e. $\phi(x)+\psi(y) \leq c(x,y)$), hence,
  the Dual Problems has a solution. Also, since $\phi^c \geq \psi$,
  then $(\phi,\phi^c)$ is also an optimal solution for the Dual, and this maximization
  problem can be restricted to searching in $c$-concave functions, i.e.:
  \begin{equation*}
    \max(\mathrm{DP}) =
    \max_{\phi \in c-conc.(X)} \int_X \phi \ d\mu +
    \int_Y \phi^c \ d\nu
  \end{equation*}
\end{prf}

\vspace{5mm}

When Strong Duality is true, the functions $\phi, \psi$ that maximize the Dual Problem
are called the \textbf{Kantorovich Potentials}.
We haven't yet proved that $\mathrm{\max(DP)}=\mathrm{\min(KP)}$, the theorem above
only gave us an idea of how the solution of the Dual Problem looks-like. Before proving
our first theorem on Strong Duality, we'll need a bit more definitions
and results.

\begin{definition}(Cyclic Monotonicity)
  For $c:X \times Y \to \overline{\mathbb R}$, a set $\Gamma \subset
    X \times Y$ is called $c$-cyclical monotone (c-CM) if
  $\forall n \in \mathbb N$ and $(x_i,y_i) \in \Gamma$ for
  $i \in \{1,...,n\}$
  \begin{equation}
    \sum^n_{i=1}c(x_i,y_i) \leq
    \sum^n_{i=1} c(x_i,y_{\sigma(i)})
  \end{equation}
  Where $\sigma(i)$ is a permutation of the indexes.
  \label{def:cyclic-monotonicity}
\end{definition}
Note that
this is a stronger property than monotonicity, since for
$n=2$ and $c(x,y) = \langle x, y \rangle$, if $\Gamma$ is c-CM,
then monotonicity is satisfied:
\begin{equation}
  \langle x_1,y_1 \rangle + \langle x_2, y_2 \rangle \leq
  \langle x_1, y_2, \rangle + \langle x_2, y_1 \rangle
\end{equation}

\begin{definition}
  For $X$	a separable metric space, we define the support of a
  a measure $\mu$ as
  \begin{equation}
    \text{spt } \mu := \bigcap
    \{
    A \ : \ A \text{ is closed and } \mu(X\setminus A) =0
    \}
  \end{equation}
\end{definition}

We can now give an overview of the proof
of first Strong Duality Theorem. The proof consists of showing
that for an optimal
plan $\gamma$, its support $\text{spt} (\gamma)$ is $c$-CM and
that for a $c$-CM set there exists a $c$-concave function
$\phi(x)$ such that $\phi(x)+\phi^c(y) = c(x,y)$ for
$(x,y) \in \text{spt}(\gamma)$. Hence, this would prove that
\begin{equation}
  \int_{X\times Y} c(x,y) \ d\gamma = \int_X \phi(x)\ d\mu + \int_Y \phi^c(y)d\nu
\end{equation}

\begin{theorem}(Santambrogio 1.37)
  If $\Gamma \neq \varnothing $ and is $c$-CM with
  $c:X\times Y \to \mathbb R$. Then,
  there exists a $c$-concave function
  $\phi:X \to \mathbb R \cup \{-\infty\}$ (different than the constant value $-\infty$) such that
  \begin{equation}
    \Gamma \subset \{
    (x,y) \ : \ \phi(x)+\phi^c(y) = c(x,y)
    \}
  \end{equation}
  In other words,
  $\forall x,y \in \Gamma, \ c(x,y) = \phi(x) + \phi^c(y)$.
  \label{thm:existsPhic}
\end{theorem}

\begin{prf}
  Fix a point $(x_0,y_0) \in \Gamma$. For $x \in X$, let
  \begin{align*}
    \phi(x) & := \inf\{
    c(x,y_n) - c(x_n,y_n) + c(x_n,y_{n-1})-c(x_{n-1},y_{n-1})+...+
    \\
            & +c(x_1,y_0)-c(x_0,y_0) \ : \ n \in \mathbb N,
    (x_i,y_i) \in \Gamma \ \forall i=1,...,n
    \}
    \\
    \\
    \psi(y) & :=
    -\inf \{
    -c(x_n,y)+c(x_n,y_{n-1}) - c(x_{n-1},y_{n-1})+...+
    \\
            & c(x_1,y_0)
    -c(x_0,y_0)) \ : \ n\in \mathbb N, (x_i,y_i) \in \Gamma \
    \forall i = 1,...,n, y_n = y
    \}
  \end{align*}
  Note that if $y \notin (\pi_y)(\Gamma)$, then there is no
  $(x_n,y) = (x_n,y_n) \in \Gamma$. Therefore,
  \begin{equation*}
    \psi(y) = -\inf\{\varnothing\} = -\infty
  \end{equation*}
  This implies that $\psi(y)> -\infty \iff y \in (\pi_y)(\Gamma)$. Note that:
  \begin{align*}
    \psi^c(x) & = \inf_y c(x,y) - \psi(y) =
    \inf_{y \in (\pi_y)(\Gamma)}
    c(x,y) - \psi(y)\\
              & =
    \inf_{y \in (\pi_y)(\Gamma)} c(x,y)
    + \inf \{
    -c(x_n,y)+... +
    +c(x_1,y_0)
    -c(x_0,y_0)) :                        \\
              & \hspace{9em}
    n\in \mathbb N, (x_i,y_i) \in \Gamma \
    \forall i = 1,...,n, y_n = y
    \}                                    \\
              & = \phi(x)
  \end{align*}

  Hence, $\phi(x)$ is $c$-concave, and $\phi(x)$ is not constantly equal to $-\infty$,
  since for $x=x_0$, we have
  \begin{align*}
     & c(x_0, y_n) + (\sum_{i=0}^{n-1} c(x_{i+1},y_{i}) ) - \sum_{i=0}^n c(x_i,y_i) \geq 0                               \\
     & \implies \phi(x_0) = \inf \{c(x_0, y_n) + (\sum_{i=0}^{n-1} c(x_{i+1},y_{i}) ) - \sum_{i=0}^n c(x_i,y_i)\} \geq 0
  \end{align*}
  Note that the inequality above is true due to the fact that $\Gamma$ is $c$-CM.

  Now, the only thing left to prove is that $\phi(x)+\phi^c(y) = c(x,y)$ for every $(x,y) \in \Gamma$.
  First, note that for $\epsilon > 0$ and $(x,y) \in \Gamma$, then:
  \begin{align*}
    &\phi(x) = \psi^c(x) = \inf_y c(x,y) - \psi(y) =
    \inf_{y \in (\pi_y)(\Gamma)} c(x,y) - \psi(y)
    \implies \\
    &\exists \bar y \in (\pi_y)(\Gamma) \ : \
    \phi(x)+ \epsilon > c(x,\bar y) - \psi(\bar y)
  \end{align*}
  Also, note that from the definition of $\psi$, we have:
  \begin{align*}
    -\psi(y) \leq -c(x,y) + c(x,\bar y) - c(\bar x_n, \bar y) + ... - c(\bar x_0, \bar y_0)
    \ : \forall i, (\bar x_i, \bar y_i) \in \Gamma
  \end{align*}
  Since this is true for any chain on $\Gamma$ starting on $\bar y$, it's true for the infimum, therefore:
  \begin{equation*}
    -\psi(y) \leq -c(x,y) + c(x,\bar y) - \psi(\bar y) \leq -c(x,y) + \phi(x) + \epsilon
  \end{equation*}

  Since the $\epsilon$ was arbitrary, we can conclude that
  $c(x,y) \leq \phi(x,y) + \psi(x)$. But, we also know that
  \begin{align*}
    \phi^c(y) = \psi^{c c}(y) & = \inf_x c(x,y) - \phi(x)                 \\
                              & = \inf_x c(x,y) - \inf_y c(x,y) - \psi(y) \\
                              & \geq \inf_x c(x,y) - c(x,y) + \psi(y)     \\
                              & = \psi(y)
  \end{align*}
  Hence, $\phi(x)+\phi^c(y) \geq \phi(x) + \psi(y) \geq c(x,y)$.

  Lastly, one would need to show that this $\phi$ is indeed measurable. The general proof is complicated,
  but, if we assume that $c$ is uniformly continuous, then, we know that $c$-transforms are continuous (this was
  shown in Theorem \ref{thm:c-conc}). Since $\phi = \psi^c$, then, $\phi$ is continuous, therefore, it is measurable
  if we consider the Borel $\sigma$-algebra.
\end{prf}

\begin{theorem} (Santambrogio 1.38)
  If $\gamma$ is an optimal transport plan for cost $c$ continuous,
  then $\text{spt } \gamma$ is $c$-CM.
  \label{thm:gamma-cCM}
\end{theorem}
\begin{prf}
  The proof consists in supposing that $\text{spt } \gamma$ is not $c$-CM. Then, we construct a
  $\tilde \gamma \in \Pi(\mu,\nu)$ such that $\int_{X\times Y} c(x,y) \ d\tilde\gamma <
    \int_{X \times Y} c(x,y) \ d\gamma$, which contradicts the optimality of $\gamma$.

  Check \citet{santambrogio2015optimal} for the complete proof.
\end{prf}

\vspace{5mm}
With these results, we can prove the first Strong Duality theorem.

\begin{theorem}
  For $X$ and $Y$ compact metric spaces, and $c:X \times Y \to
    \overline{\mathbb R}$ continuous. Then, $\max\mathrm{(DP)} = \mathrm{\min(KP)}$,
    and DP admits a solution $(\phi,\phi^c)$.
  \label{thm:compactstrongduality}
\end{theorem}
\begin{prf}
  Using Theorem \ref{thm:Santambrogio1.4}, we obtain that $\exists \gamma \in \Pi(\mu,\nu)$
  such that it minimizes the Kantorovich Problem, therefore, by Theorem \ref{thm:gamma-cCM},
  $\text{spt}\gamma$ is $c$-CM.

  By Proposition \ref{thm:c-conc}, we know that a solution to the Dual Problem
  can be found in the set of $c$-concave functions.
  Using \ref{thm:existsPhic}, we can assert that there is a set of $c$-concave
  functions such that $\phi(x)+\phi^c(y) = c(x,y)$ for every $(x,y) \in \text{spt }\gamma$.
  Since $X\times Y$ is compact, then $c$ is uniformly compact, which implies that
  $\phi$ and $\phi^c$ are continuous and bounded.

  Hence, since we already know that $\mathrm{\max(DP)} \leq \mathrm{\min(KP)}$, we conclude that
  $\mathrm{\max(DP)} = \mathrm{\min(KP)}$.
\end{prf}

\begin{theorem}
  For $X$ and $Y$ Polish spaces and $c:X\times Y \to \mathbb R$ uniformly continuous and bounded. Then,
  (DP) admits a solution $(\phi,\phi^c)$ and $\mathrm{\max(DP)}=\mathrm{\min (KP)}$.
  \label{thm:polishStrongDuality}
\end{theorem}

\begin{prf}
  First, note that since $X$ and $Y$ are Polish and $c$ is continuous,
  one can use Theorem \ref{thm:existanceKPpolish} and affirm that exists an optimal solution
  $\gamma$ to (KP).

  By the same arguments used on the proof of Theorem \ref{thm:compactstrongduality},
  we stablish that $\text{spt } \gamma$ is $c$-CM, and that $\phi, \phi^c$ are continuous functions
  such that $\forall (x,y) \in \text{spt } \gamma$, $\phi(x) + \phi^c(y) = c(x,y)$.

  In the Dual Problem, the admissible functions $\phi$ and $\psi$ must be continuous and bounded. Hence,
  we just need to prove that the $\phi$ and $\phi^c$ are indeed bounded. Note that, since $c$ is bounded,
  then, $|c| \leq M \in \mathbb R$ and
  \begin{equation*}
    \phi^c(y) = \inf_x c(x,y) - \phi(x) \leq  \inf_x M - \phi(x) =
    M - \sup_x \phi(x)
  \end{equation*}
  Note that in $\ref{thm:existsPhic}$, we showed that $\phi$ is not constantly $-\infty$. Therefore,
  \begin{equation*}
    -\infty < L < \sup_x \phi(x) \implies
    \phi^c(y) \leq M - \sup_x \phi(x) \leq M - L
  \end{equation*}
  Similarly, since $\phi = \psi^c$ and $\phi^c(y)\geq \psi(y)$ (shown in \ref{thm:gamma-cCM}), then:
  \begin{align*}
    \phi(x) = \inf_y c(x,y) - \psi(y) \geq - M - \sup_y \psi(y) & \geq - M - \sup_y \phi^c(y) \\
                                                                & \geq -M - M + L
  \end{align*}

  Hence, we obtained an upper bound for $\phi^c$ and a lower bound for $\phi$. Now, we obtain an upper bound
  for $\phi$ and a lower bound for $\phi^c$ using a similar argument and relying on the fact that
  $\sup \psi(y) > L > -\infty$:
  \begin{align*}
    \phi(x)  & = \inf_y c(x,y) - \psi(y) \leq M - \sup_y \psi(y) \leq M - L        \\
    \phi^c(x) & = \inf_x c(x,y) - \phi(x) \geq - M - \sup_x \phi(x) \geq -M - M - L
  \end{align*}

  Finally, using the same arguments as Theorem \ref{thm:compactstrongduality}, we conclude that
  $\mathrm{\max (DP)} = \mathrm{\min (KP)}$ and that $(\phi,\phi^c)$ are a solution for the Dual Problem.
\end{prf}

One cost that is of special interest is the quadratic cost $\frac{1}{2} |x-y|^2$. Note that
this cost is neither bounded nor uniformly continuous for non-compact metric spaces. Hence, the previous
theorems do not address it. But one can still prove that Strong Duality is true for such case.

\begin{theorem}
  Let $\mu, \nu \in \mathcal P (\mathbb R^d)$, with $c(x,y) = \frac{1}{2} |x-y|^2$. Suppose that
  $\int|x|^2 d\mu, \int|y|^2 d\nu < +\infty$
  \footnote{This is Theorem 1.40 in \citet{santambrogio2015optimal}, but note that there is a small typo in the book,
    where it states $\int|x|^2 dx, \int|y|^2 dy < + \infty$ instead of the correct $\int|x|^2 d\mu, \int|y|^2 d\nu < +\infty$.}
  . Instead of the original Dual Problem, consider the
  following formulation:
  \begin{flalign}
    \mathrm{(DP')} &&
    \sup \left \{
    \int_{\mathbb R^d} \phi \ d\mu + \int_{\mathbb R^d} \psi \ d\nu \ :
    \phi \in L ^1(\mu) \ , \psi \in L ^1(\nu) \ ,
    \ \phi \oplus \psi \leq c
    \right \}
    &&
    \label{eqt:dualproblemvar}
  \end{flalign}
  Therefore, (DP') admits a solution $(\phi,\psi)$ and $\mathrm{\max (DP')} = \mathrm{\min (KP)}$.
\end{theorem}
\begin{prf}
  First, in the same way as the proof of Theorem \ref{thm:polishStrongDuality}, (KP) has an optimal solution $\gamma$
  with $\text{spt }\gamma$ that is $c$-CM and $ \forall (x,y) \in \text{spt } \gamma$ we have $\phi(x)+ \psi(y) = c(x,y)$.
  We also have that $-\psi(y)=-\phi^c(y)=\sup_x - \frac{|x-y|^2}{2} + \phi(x)$.
  Note that, for $h(x) := \frac{|x|^2}{2} -\phi(x)$
  \begin{align*}
    h^*(y):=\sup_x \langle x,y \rangle - h(x) =
    \sup_x \langle x,y \rangle - \frac{|x|^2}{2}  + \phi(x) &= \\
    \frac{|y|^2}{2} + \sup_x -\frac{|x-y|^2}{2} + \phi(x) = \frac{|y|^2}{2} - \psi(y)
  \end{align*}
  Therefore, $h(x)$ is equal to the Legendre-Fenchel transform of
  $\frac{|y|^2}{2} + \psi(y)$, which implies that $h$ is convex l.s.c. The same argument can be used
  to show that $\frac{|y|^2}{2} - \psi(y)$ is also convex l.s.c.

  Since $\frac{|x^2|}{2} - \phi(x)$ is convex, there exists a supporting hyperplane, hence, it
  is bounded from below by a linear function, which implies that
  \begin{align*}
    \frac{|x^2|}{2} - \phi(x) \geq \alpha \langle x,y \rangle  + \beta & \implies
    \phi(x) \leq \frac{|x^2|}{2} - \alpha \langle x,y \rangle - \beta             \\
                                                                       & \implies
    \int_{\mathbb R^d} \phi(x) \ d\mu \leq \int_{\mathbb R^d} \frac{|x^2|}{2} - \alpha \langle x,y \rangle - \beta \ d\mu < +\infty
  \end{align*}

  The same argument can be made for $\psi$, which means that $\phi_+ \in  L^1(\mu)$ and $\psi_+ \in L^1(\nu)$.
  Due to the fact that $\phi(x) + \psi(y) = c(x,y)$ in the support of $\gamma$, then
  \begin{equation*}
    \int_{\mathbb R^d \times \mathbb R^d} \phi \oplus \psi \ d\gamma  =
    \int_{\mathbb R^d \times \mathbb R^d} c \ d\gamma  \geq 0
  \end{equation*}
  Which implies that the negative portions of $\phi$ and $\psi$ are also integrable, leading us to conclude
  that $\phi \in L^1(\mu)$ and $\psi \in L^1(\nu)$.

  Finally, by the same arguments as the previous theorems, we prove that
  $\mathrm{\max(DP')}= \mathrm{\min(KP)}$.

\end{prf}

\vspace{5mm}
A stronger result can be proven regarding the duality of KP. We'll present it here without a proof.
\begin{theorem}(Santambrogio 1.42)
  For $X$ and $Y$ Polish spaces and $c:X\times Y \to \mathbb R\cup \{+\infty\}$ l.s.c and bounded from below. Then,
  $\mathrm{\sup(DP)}=\mathrm{\min (KP)}$.
  Note that in this theorem, one cannot guarantee the existence of the $(\phi,\psi)$ that maximize the Dual Problem.
  \label{thm:strongerDuality}
\end{theorem}


If the cost $c(x,y)$ is actually a distance metric (Def. \ref{def:metric}),
then we can prove the following result:
\begin{theorem}
  Let $X$ be a metric space, and $c:X \times X \to \mathbb{R}$, where $c$ is a distance metric. Therefore,
  a function $f:X \to \mathbb{R}$ is $c$-concave if and only if it is Lipschitz continuous with a constant
  less than 1 with respect to the distance $c$.
  We call $\text{Lip}_1^{(c)}$ this set of Lipschitz functions with constant less than 1. Moreover,
  $f^c = -f$.
  \label{thm:cConcaveLip1}
\end{theorem}
\begin{prf}

  $\implies$) Let $f:X \to \mathbb R$ be a $c$-concave function. Hence, $\exists \ g:X \to \overline{\mathbb R}$ such that
  \begin{equation*}
    f(x) := \inf_y c(x,y) - g(y)
  \end{equation*}
  Using the triangle inequality of the cost, we get:
  \begin{gather*}
    c(x,y) \leq c(x,z) + c(z,y) \implies \sup_y c(x,y) - c(y,z) \leq c(x,z) \\
    c(y,z) \leq c(y,x) + c(x,z) \implies \sup_y c(y,z) - c(x,y) \leq c(x,z) \\
    \therefore \\
    \sup_y |c(y,z) - c(x,y)| \leq c(x,z)
  \end{gather*}
  Therefore,
  \begin{align*}
    |f(x) - f(z)| & = |\inf_y \{c(x,y) - g(y) \} \ - \ \inf_y \{c(z,y) - g(y)\}| \leq \\
                  & \underset{\ref{lem:infsup_ineq}}{\leq}
    \sup_y |c(x,y) - c(z,y)| \leq c(x,z)
  \end{align*}

  $\impliedby$) Let $f \in \text{Lip}^{(c)}_1$. Using the Lipschitz inequality,
  \begin{equation*}
    f(x) - f(y) \leq c(x,y) \implies f(x) \leq \inf_y c(x,y) + f(y)
  \end{equation*}
  But note that $f(x) = c(x,x) + f(x) \geq \inf_y c(x,y) - f(y)$. This implies that
  $f(x) = \inf_y c(x,y) + f(y)$. Hence, $f(x) = g^c(x)$, where $g(y) = -f(y)$. Which proves
  that $f$ is $c$-concave, and $f = (-f)^c$. Finally, note that $-f$ is also $\text{Lip}_1$,
  therefore, the same argumentation leads to $-f = f^c$.
\end{prf}
\vspace{5mm}

Lastly, using  Theorems
\ref{thm:strongerDuality} and \ref{thm:cConcaveLip1}, one obtains the famous
Kantorovich-Rubinstein Duality:

\begin{theorem}(Kantorovich-Rubinstein)

  Let $(X,d)$ be a Polish space with metric $d$, and cost function $c(x,y) = d(x,y)$.
  Then, for $\mu, \nu \in \mathcal P(X)$, the Kantorovich Problem
  is equivalent to
  \begin{equation}
      \sup \left \{
      \int_X \phi \ d\mu - \int_X \phi \ d\nu \ :
      \phi \in Lip_1(X)
      \right \}
  \end{equation}
  \label{thm:Kantorovich-Rubinstein}
\end{theorem}

\newpage
\chapter{Wasserstein Distance}

In this chapter we focus on how the minimal transport cost can be used as a distance metric
in the space of probability measures. Let's assume that $(X,d)$ is a Polish metric space,
$d$ is a lower semi-continuous metric on this space and $ p \in [1,+\infty)$.

\begin{definition}(Probability space with p-Moments)

  \begin{equation}
    \mathcal P_p(X) := \{
         \mu \in \mathcal P(X): \int_{X \times X} d(x,y)^p \ d \mu(x) d \mu(y) < +\infty
      \}
  \label{eq:Pp}
  \end{equation}
  Note that this is equivalent to the set of probability measures such that $\int_X d(x,x_0) \ d\mu<+\infty$
  for every $x_0 \in X$. The proof of this statement can be found
  in \citet{garling2018analysis} Proposition 21.1.1.
\end{definition}

\begin{definition}(Wasserstein Distance)

  Let $(X,d)$ be a Polish metric space, with $c:X \times X \to \mathbb R$ such that $c(x,y)=d(x,y)^p$, and
  $p \in [1,+\infty)$.
  For $\mu,\nu \in \mathcal P_p(X)$, the Wasserstein Distance is given by:
  \begin{equation}
    W_p(\mu,\nu) :=
    \left(
    \inf_{\gamma \in \Pi(\mu,\nu)}
    \int_{X \times X} d(x,y)^p \ d\gamma
    \right)^{1/p}
    \label{def:Wasserstein}
  \end{equation}
  Note that the restriction to $\mu,\nu \in \mathcal P_p(X)$ is necessary for $W_p$ to be a distance metric.
  Moreover, for $p=1$, then $c(x,y) = d(x,y)$ is a metric on $X$, therefore, for $X$ Polish, one can
  use Kantorovich-Rubinstein's Duality Theorem \ref{thm:Kantorovich-Rubinstein} to obtain:
  \begin{equation}
    W_1(\mu,\nu) =
    \sup_{\phi \in Lip_1} \int_X f d (\mu - \nu)
  \end{equation}
\end{definition}

Let's prove that $W_p$ indeed is a metric on $\mathcal P_p(X)$.

\begin{lemma} (Gluing Lemma)

  Let $(X,d)$ be a metric space. For $\mu,\nu,\rho \in \mathcal P(X)$ and
  $\gamma^+ \in \Pi(\mu,\rho)$, $\gamma^- \in \Pi(\rho,\nu)$. Then,
  $\exists \ \sigma \in \mathcal P(X \times X \times X)$ such that
  $(\pi_{x,y})_\# \sigma  = \gamma^+$.
  $(\pi_{y,z})_\# \sigma  = \gamma^-$.
  \label{lem:gluing}
\end{lemma}
\begin{prf}
  First, use disintegration (Def. \ref{def:disintegration}) with respect to $f = \pi_y$ to obtain $\gamma^+_y$ and $\gamma^-_y$.
  We know that such
  disintegration exists and is essentially unique since $X$ is Polish (see Theorem \ref{thm:disintegrationunique}).
  Note that disintegrated measures are actually
  defined on $X \times \{y\} \subset X \times X$, but, by abuse of notation, we'll consider that they
  are measures on $X$, and $y$ is only an index.

  Therefore, make $\sigma = \gamma_y^+ \otimes \rho \otimes \gamma_y^-$, and let $\phi:X \times X \to \mathbb R$
  be a measurable function.
  Hence:
  \begin{align*}
    \int_{X \times X \times X} \phi(x,y) \ d\sigma & \overset{\text{Fubini}}{=}
    \int_X \int_X \int_X \phi(x,y) \
    d\gamma_y^+(x) \otimes \rho(y) \otimes \gamma_y^-(z)                                        \\
                                                                  & \overset{\text{Indep.}}{=}
    \int_X d\gamma_y^-(z) \ \int_X \int_X \phi(x,y) \
    d\gamma_y^+(x) \otimes \rho(y)                                                              \\
                                                                  & \overset{\text{Disint.}}{=}
    \int_X d\gamma_y^-(z) \ \int_{X \times X} \phi(x,y) \
    d\gamma^+(x,y)                                                                              \\
                                                                  & \overset{\text{\hfill}}{=}
    \int_{X\times X} \phi(x,y) \
    d\gamma^+(x,y)
  \end{align*}
  Since $\phi(x,y)$ is arbitrary, then by Corollary \ref{cor_marginals}, we can conclude that
  $(\pi_{x,y})_\# \sigma  = \gamma^+$. By the same argument, we obtain
  $(\pi_{y,z})_\# \sigma  = \gamma^-$, which concludes our proof.

\end{prf}

\begin{proposition}
  $W_p(\cdot,\cdot)$ is a metric on $\mathcal P_p(X)$.
\end{proposition}
\begin{prf} Let's prove each of the three properties that categorize a metric.

  \vspace{5mm}
  \noindent	i) $d(x,y) = 0 \iff x = y$.

  \vspace{5mm}
  \noindent
  If $\mu=\nu$, then
  $(id,id)_\# \mu = \gamma$, hence $\int_{X \times X} d(x,y)^p
    \ d\gamma = \int_{X \times X} d(x,x)^p \ d\mu =0$.

  \vspace{5mm}
  If $W_p(\mu,\nu)=0$, then $\int_{X\times X}d(x,y)^p \ d\gamma = 0$. Therefore, $\gamma$ is concentrated
  on $\{x=y\}$, otherwise, there would exist a set $\ A \times B$ such that $\gamma(A \times B)>0$ and $x\neq y$.
  Therefore $\int_X d(x,y)^p d\gamma >0$.

  Since $\gamma$ is concentrated on $\{x=y\}$, then for any set Borel set $K \subset X$:
  \begin{equation*}
    \gamma(K) = \int_{X\times X} \mathbbm 1_K(x,y) \ d\gamma =
    \int_{x=y} \mathbbm 1_K(x,y) \ d\gamma = \int_{x=y} \mathbbm 1_K(x) \ d\mu
    = \int_{x=y} \mathbbm 1_K (y) \ d\nu
  \end{equation*}
  We can conclude that $\mu(K)=\nu(K)$ for every Borel set $K$, therefore $\mu=\nu$ almost everywhere.

  \vspace{5mm}

  \noindent	ii) $d(x,y)=d(y,x)$.
  \begin{equation*}
    W_p(\mu,\nu) = \left(\int_{X \times X} d(x,y)^p d\gamma \right)^{1/p} =
    \left(\int_{X \times X} d(y,x)^p d\gamma\right)^{1/p} = W_p(\nu,\mu)
  \end{equation*}

  \vspace{5mm}
  \noindent	iii) $d(x,z) \leq d(x,y) + d(y,z)$.

  Let $\mu,\nu,\rho \in \mathcal P_p(X)$, and $\gamma^+ \in \Pi(\mu,\rho)$, $\gamma^- \in \Pi(\rho,\nu)$ are
  the optimal transport plans for the respective measures.
  Using the Gluing Lemma \ref{lem:gluing}, we know that there exists a measure
  $\sigma \in \mathcal P(X \times X \times X)$, where
  $(\pi_{x,y})_\# \sigma = \gamma^+$ and
  $(\pi_{y,z})_\# \sigma = \gamma^-$. Also, let $\gamma := (\pi_{x,z})_\# \sigma$. Hence,
  \begin{align*}
    W_p(\mu,\nu) & \quad \leq
    \left(
      \int_{X \times X} d(x,z)^p \ d\gamma
    \right)^{1/p} =
    \left(
      \int_{X \times X} d(x,z)^p \ d(\pi_{x,z})_\# \sigma
    \right)^{1/p}\\
    & \underset{Thm. \ref{thm:pushforward}}{=}
    \left(
    \int_{X \times X \times X} d(x,z)^p \ d \sigma
    \right)^{1/p}\\
    & \quad \leq
    \int_{X^3} (d(x,y)+d(y,z))^p \ d \sigma \\
    &=
    ||
    d \circ (\pi_{x,y})(x,y,z) -
    d \circ (\pi_{y,z})(x,y,z)
    ||_{L^p(\sigma)} \\
    & \underset{\ref{lem:minkowski}}{\leq}
    ||
    d \circ (\pi_{x,y})(x,y,z)
    ||_{L^p(\sigma)} +
    ||
    d \circ (\pi_{y,z})(x,y,z)
    ||_{L^p(\sigma)} \\
    & =
    \left(
      \int_{X^3} d(x,y)^p \ d\sigma
    \right)^{1/p} +
    \left(
      \int_{X^3} d(y,z)^p \ d\sigma
    \right)^{1/p} \\
    & =
    \left(
      \int_{X^2} d(x,y)^p \ d\gamma^+
    \right)^{1/p} +
    \left(
      \int_{X^2} d(y,z)^p \ d\gamma^-
    \right)^{1/p} \\
    & =
    W_p(\mu,\rho) + W_p(\rho,\nu)
  \end{align*}
  Which proves the triangle inequality for the Wasserstein distance.

\end{prf}

\begin{definition} (Wasserstein Space)
  For a Polish space $X$, we call $\mathcal P_p(X)$ a Wasserstein space if it is endowed with
  the p-Wasserstein metric. Note that is also common to see this space symbolized by $\mathcal W_p(X)$.
\end{definition}

\begin{proposition}
  For a bounded Polish space $X$, $p \in [1,+\infty)$, $\mu,\nu \in \mathcal P_p(X)$ and $C\in \mathbb R_+$, then
  \begin{equation}
    W_1(\mu,\nu) \leq W_p(\mu,\nu) \leq CW_1(\mu,\nu)^{1/p}
  \end{equation}
  \label{prop:ineqwasserstein}
\end{proposition}
\begin{prf}
  Let $p\leq q \in [1,+\infty)$ and $\gamma \in \Pi(\mu,\nu)$. Hence, $\phi(x)=x^{q/p}$ is a convex function, so by Jensen's
  inequality:
  \begin{align*}
    \phi\left(
    \int d(x,y)^p d\gamma
    \right)^{1/q} =
    \left(
    \int d(x,y)^p d\gamma
    \right)^{1/p}
     & \leq
    \left(
    \int \phi(d(x,y)^p) d\gamma
    \right)^{1/q} \\
     & =
    \left(
    \int (d(x,y)^q) d\gamma
    \right)^{1/q}
  \end{align*}
  This implies that $W_p(\mu,\nu) \leq W_q(\mu,\nu)$, when $p\leq q$. In particular,
  $W_1(\mu,\nu)\leq W_p(\mu,\nu)$ for $p\geq 1$.

  Now, since $X$ is bounded, then $d(x,y) \leq \sup_{x,y \in X}d(x,y) = d(X)$. Hence,
  \begin{gather*}
    d(x,y)^p \leq d(X)^{p-1}d(x,y) \\
    \therefore
    \\
    \left(
    \int d(x,y)^p d\gamma
    \right)^{1/p} \leq
    \left(
    \int d(x,y) d\gamma
    \right)^{1/p}d(X)^{\frac{p-1}{p}}
  \end{gather*}
  Therefore, we conclude that $W_p(\mu,\nu)\leq d(X)^{\frac{p-1}{p}} W_1(\mu,\nu)^{1/p}$

\end{prf}

Next, let's present some of the topological properties of such space.	A first thing to note is that
for probability spaces, the notion of weak convergence can be made more strict with the following lemma:

\begin{lemma}
  For a space of probability measures, we say that $\mu_n$ converges weakly to $\mu$, i.e.
  $\mu_n \rightharpoonup \mu \iff \ \forall f \in C_c(X), \ \int f \ d\mu_n \to \int f \ d\mu$, where
  $C_c(X)$ is the space of continuous functions with compact support. Note that
  $C_c(X) \subset C_0(X) \subset C_b(X)$.
  \label{lem:weakconvergenceCc}
\end{lemma}
\begin{prf}

  $\implies)$ If $\mu_n \rightharpoonup \mu$	, then $f \in C_c(X)\subset C_b(X)$, hence $\int f d\mu_n \to \int f d\mu$.

  \vspace{5mm}
  $\impliedby)$ Suppose that $\forall f \in C_c(X),\ \int f d\mu_n \to \int f d\mu$. Hence, note that for
  any constant $M$, $\int f + M d\mu_n = \int f d\mu_n + C \to \int f d\mu + C$.
  Take $g \in C_b(X)$ and make $g' = g + C \geq 0$ and
  $g' \mathbbm 1_{[-k,k]} = f_k \in  C_c(X)$. Which implies that $f_k \uparrow g'$.
  Now,
  \begin{align*}
    \left|\int g d\mu_n - \int g d\mu \right| & =
    \left|\int g' d\mu_n - \int g' d\mu \right|      \\
                                              & \leq
    \left|\int g' d\mu_n - \int f_k d\mu_n \right| +
    \left|\int f_k d\mu_n - \int f_k d\mu \right| +
    \left|\int f_k d\mu - \int g' d\mu \right|
  \end{align*}
  Since $f_k \in C_c(X)$, then for $n$ big enough,
  $\left|\int f_k d\mu - \int f_k d\mu_n \right|< \epsilon$. Therefore,
  \begin{align*}
    \left|\int g d\mu_n - \int g d\mu \right| \leq
    \left|\int g' d\mu_n - \int f_k d\mu_n \right| +
    \epsilon +
    \left|\int f_k d\mu - \int g' d\mu \right|
  \end{align*}
  Since $f_k \uparrow g'$, then,
  by the Monotone Convergence Theorem,

  \begin{gather*}
    \lim_{k\to +\infty}
    \left|\int g' d\mu_n - \int f_k d\mu_n \right| = 0 \\
    \lim_{k\to +\infty}
    \left|\int f_k d\mu - \int g' d\mu \right| = 0 \\
    \therefore
  \end{gather*}

  \begin{equation*}
    \lim_{k\to +\infty}\left|\int g d\mu_n - \int g d\mu \right| =
    \left|\int g d\mu_n - \int g d\mu \right| \leq
    \epsilon
  \end{equation*}
\end{prf}

\begin{theorem}
  Let $(X,d)$ be a Polish compact space with $\mu_n,\mu \in P_p(X)$ and
  $p \in [1,+\infty)$, then $W_p(\mu_n,\mu)\to 0 \iff \mu_n \rightharpoonup \mu$.
  \label{thm:compactwassersteinconv}
\end{theorem}
\begin{prf}

  $\implies)$ Let $W_p(\mu_n,\mu)\to 0$. Since $X$ is compact and $c$ is a continuous function,
  by Theorem \ref{thm:Santambrogio1.4} the Kantorovich Problem has a solution. Also, by Theorem \ref{thm:compactstrongduality},
  we obtain that $\max(\mathrm{DP})=\min(\mathrm{KP})$. First, we prove for $p=1$.
  In this case, using the Lipschitz version of DP:
  \begin{equation*}
    W_1(\mu,\nu)=
    \max \left \{
    \int_X \phi \ d\mu - \int_X \phi \ d\nu \ :
    \phi \in Lip_1(X)
    \right \} \to 0
  \end{equation*}
  This implies that for any $f \in \text{Lip}_1, \int f d\mu_n \to \int f d\mu$. Note that, by linearity,
  the same is true for any Lipschitz function. Since $X$ is compact, then Lipschitz functions are
  dense on $C(X)$ (see Theorem \ref{thm:lipdense}), which leads us to conclude that $\mu_n \rightharpoonup \mu$
  (by Portmanteau \ref{Portmanteau}). Now, by Proposition \ref{prop:ineqwasserstein},
  the same is valid for any $p\geq 1$.

  $\impliedby)$ Let $\mu_n \rightharpoonup \mu$. Define a subsequence $\mu_{n_k}$ such that
  $\lim_k W_1(\mu_{n_k},\mu)=\limsup_n W_1(\mu_n,\mu)$. By the same arguments already used,
  we know that for each $\mu_{n_k}$ there is a $\phi_{n_k} \in \text{Lip}_1$ such that
  $W_1(\mu_{n_k},\mu) = \int_X \phi_{n_k}d(\mu_{n_k}-\mu)$.

  For an arbitrary $\epsilon >0$, make $\delta = \epsilon$. Since $\phi_{n_k}$ is 1-Lipschitz,
  if $d(x,y) < \delta$, then
  $|\phi_{n_k}(x) - \phi_{n_k}(y)| \leq d(x,y) < \epsilon, \ \forall k \in \mathbb N$. Therefore, the sequence is
  Equicontinuous.

  Also, for $x_0 \in X$, we can make $\phi_{n_k}'(x):= \phi_{n_k}(x) - \phi_{n_k}(x_0)$. Note that these functions are
  1-Lipschitz and still satisfy
  $W_1(\mu_{n_k},\mu) = \int_X \phi_{n_k}'d(\mu_{n_k}-\mu)$. Hence, let's use $\phi_{n_k}'$ as our new subsequence.
  In this case,
  \begin{equation*}
    |\phi_{n_k}'(x)| =
    |\phi_{n_k}(x) - \phi_{n_k}(x_o)|
    \leq d(x,x_o) \leq d(X) <+\infty
  \end{equation*}
  This implies that this sequence of $\phi_{n_k}'$ is Equibounded.
  With this, we can use Arzelà-Ascoli Theorem (\ref{thm:arzela-ascoli})
  to obtain a sub-subsequence that converges uniformly to a $\phi \in \text{Lip}_1(X)$.
  Replace and relabel the original subsequence, obtaining:
  \begin{align*}
     & W_1(\mu_{n_k},\mu) = \int_X \phi_{n_k}d(\mu_{n_k}-\mu) \\
     & =
    \left|
    \int_X \phi_{n_k}d\mu_{n_k}+
    \int_X \phi d\mu_{n_k} -
    \int_X \phi d\mu_{n_k} +
    \int_X \phi d\mu -
    \int_X \phi d\mu -
    \int_X \phi_{n_k}d\mu
    \right|                                                        \\
     & \leq
    \underbrace{
      \left|
      \int_X \phi_{n_k}d\mu_{n_k} -
      \int_X \phi d\mu_{n_k}
      \right|}
    _{\text{Goes to }0, \text{due to } \phi_{n_k}\to_u \phi}+
    \underbrace{
      \left|
      \int_X \phi d\mu-
      \int_X \phi_{n_k} d\mu
      \right|}
    _{\text{Goes to }0, \text{due to } \phi_{n_k}\to_u \phi}+
    \underbrace{
      \left|
      \int_X \phi d\mu_{n_k} -
      \int_X \phi d\mu
      \right|}
    _{\text{Goes to }0, \text{due to } \mu_{n_k}\rightharpoonup \mu}
  \end{align*}

  Therefore $\limsup_n W_1(\mu_n,\mu) \leq 0 \implies W_1(\mu_n\mu) \to 0$. To conclude the proof
  for any $p \in [1,+\infty)$, we use Proposition \ref{prop:ineqwasserstein}:
  \begin{equation*}
    0 \leq W_p(\mu_n,\mu) \leq CW_1(\mu_n,\mu)^{1/p} \leq 0
  \end{equation*}
\end{prf}

\begin{theorem}
  For $X \subset \mathbb R^n$, $\mu_n,\mu \in \mathcal P_p(X)$, $x_0 \in X$ and
  $d$ is metric on $X$, then
  \begin{equation}
    W_p(\mu_n,\mu) \to 0 \iff \int_X d(x,x_0)^p d\mu_n \to \int_X d(x,x_0)^p d\mu
    \text{ and } \mu_n \rightharpoonup \mu
  \end{equation}
  \label{thm:convwasserstein}
\end{theorem}

\begin{prf}

  $\implies)$ Let $W_p(\mu_n,\mu)\to 0$. Since $X$ is Polish, and $c$ is a continuous function,
  by Theorem \ref{thm:existanceKPpolish} the Kantorovich Problem has a solution. Also, by Theorem \ref{thm:strongerDuality},
  we obtain that $\sup(\mathrm{DP})=\min(\mathrm{KP})$. We know that
  $W_p(\mu_n,\mu) \geq W_1(\mu_n,\nu)\geq 0$, hence, using the Lipschitz version of the Dual Problem for $W_1$:
  \begin{equation*}
    \sup \left \{
    \int_X \phi \ d\mu_n - \int_X \phi \ d\mu \ :
    \phi \in Lip_1(X)
    \right \} \to 0
  \end{equation*}

This implies that for any $f \in \text{Lip}_1, \int f d\mu_n \to \int f d\mu$. Note that, by linearity,
the same is true for any Lipschitz function, not only $\text{Lip}_1$. Finally, since Lipschitz functions are
dense on $C_c(X)$ (see Theorem \ref{thm:lipdense}),
we can use Lemma \ref{lem:weakconvergenceCc} to conclude that $\mu_n \rightharpoonup \mu$.

To prove the other condition (i.e.
$\int_X d(x,x_0)^p d\mu_n \to \int_X d(x,x_0)^p d\mu$),
define $\delta_{x_0}$ as a measure with mass on $x_0$. Which means that the optimal transport plan
$\gamma_n$ is in $\Pi(\mu_n,\delta_{x_0})$. This implies that $\gamma_n(x,y) = 0$ for any $y \neq x_0$. Therefore
\begin{align*}
  W_p(\mu_n,\delta_{x_0})^p = \int_{X \times X} d(x,y)^p d\gamma_n & = \int_{X \times \{x_0\}} d(x,y)^p d\gamma_n                              \\
  & = \int_X d(x,x_0)^p d\mu_n \to W_p(\mu,\delta_{x_0})^p = \int_X d(x,x_0)^p d\mu
\end{align*}
Where we used the fact that $W(\mu_n, \delta_{x_0}) \to W(\mu,\delta_{x_0})$, which is true since
$W(\mu_n,\delta_{x_0}) - W(\mu,\delta_{x_0}) \leq W(\mu_n,\mu)$.

\vspace{5mm}
$\impliedby)$ Consider now that $\mu_n \rightharpoonup \mu$ and
Define $\pi_R :X \to \overline{\text{B}(R)}$, which is the projection on the closed ball with radius $R$
centered at $x_0$. Since $W_p(\cdot,\cdot)$ is a metric, we have:
\begin{gather*}
  W_p(\mu_n,\mu) \leq
  W_p(\mu_n,(\pi_R)_\#\mu_n) +
  W_p((\pi_R)_\#\mu_n,(\pi_R)_\#\mu)+
  W_p((\pi_R)_\#\mu_n,\mu)
\end{gather*}

For sake of clarity in the proof, let's define, without loss of generalization, that $d(x,x_0) = |x|$ and
$d(x,y) = |x-y|$.
Now, note that $|x - \pi_R(x)| =|x| - |x| \wedge R$ and
that the plan $(id,\pi_R)_\# \mu$ is a possible solution to the OT Problem of transporting
$\mu$ to $(\pi_R)_\#\mu$. Therefore:
\begin{align*}
  W_p(\mu,(\pi_R)_\# \mu)^p & \leq
  \int_{X \times X} |x-y|^p d (id,\pi_R)_\# \mu =
  \int_{(id,\pi_R)^{-1}(X\times X)} |x-\pi_R(x)|^p d\mu \\
                            & =
  \int_{X} |x-(x\wedge R)|^p d\mu =
  \int_{B(R)^c} (|x|-R)^p d\mu                                    \\
\end{align*}
And using the same arguments:
\begin{align*}
  W_p(\mu_n,(\pi_R)_\# \mu_n)^p & \leq
  \int_{B(R)^c} (|x|-R)^p d\mu_n
\end{align*}
Now, note that
\begin{equation*}
  \int_X |x|^p - (|x|\wedge R)^p d\mu = \int_{B(R)}|x|^p -|x|^p d\mu
  + \int_{B(R)^c} |x|^p - R^p d\mu \leq \int_{B(R)^c} |x|^p d\mu
\end{equation*}
Since $\mu_n, \mu \in \mathcal P_p(X)$, we know that
$\int_{X}|x|^p d\mu = C < +\infty$ and
$\int_{X}|x|^p d\mu_n = C < +\infty$ then
\begin{align*}
  \int_{B(R)^c} |x|^p d\mu = C - \int_{B(R)}|x|^p d\mu \quad \therefore \quad
  \lim_{R \to 0}
  \int_{B(R)^c}|x|^p = 0
\end{align*}
Using that $(|x|- R)^p \leq |x|^p - (|x|\wedge R)^p$ for every $x \in B(R)^c$, we get
\begin{align*}
  W_p(\mu_n,(\pi_R)_\# \mu)^p \leq
  \int_{B(R)^c} (|x|-R)^p d\mu_n \leq
  \int_{B(R)^c} |x|^p-R^p d\mu_n \leq
  \int_{B(R)^c}|x|^p
\end{align*}
Now, note that since $\int|x|^p \mu_n \to \int|x|^p d\mu$ and that $(|x|\wedge R)$ is continuous and bounded,
\begin{align*}
  \lim_n
  W_p(\mu_n,(\pi_R)_\#\mu_n) & \leq
  \lim_n
  \int_{B(R)^c} (|x|-R)^p d\mu_n    \\
                             & \leq
  \lim_n
  \int_{B(R)^c} |x|^p-R^p d\mu_n =
  \int_{B(R)^c} |x|^p-R^p d\mu \leq
  \int_{B(R)^c} |x|^p d\mu
\end{align*}
Hence,
\begin{align*}
  \lim_R \lim_n (W_p(\mu_n,(\pi_R)_\#\mu_n) \leq \lim_R
  \int_{B(R)^c} |x|^p d\mu = 0 \\
  \lim_R (W_p(\mu,(\pi_R)_\#\mu) \leq \lim_R
  \int_{B(R)^c} |x|^p d\mu = 0
\end{align*}

Lastly, note that since $\overline{B(R)}$ is compact, then we can use Theorem \ref{thm:compactwassersteinconv}
to stablish that
\begin{equation*}
  \lim_n W_p((\pi_R)_\#\mu_n,(\pi_R)_\#\mu) = 0
\end{equation*}

We can then conclude that
\begin{align*}
  \limsup_n W_p(\mu_n,\mu) &\leq
  \lim_R \limsup_n (
  W_p(\mu_n,(\pi_R)_\#\mu_n) \\ & \quad \quad+
  W_p((\pi_R)_\#\mu_n,(\pi_R)_\#\mu)\\ & \quad \quad+
  W_p((\pi_R)_\#\mu_n,\mu)) \\
  &= 0
\end{align*}

\end{prf}

The Theorem above was proved for $X \subset \mathbb R^d$,
but a more general result can be proven for Polish spaces. Such result is presented below without a proof.
The proof can be found in \citet{villani2008optimal} under Theorem 6.9.

\begin{theorem}

  For $(X,d)$ a Polish metric space, $\mu_n,\mu \in \mathcal P_p(X)$ and $x_0 \in X$. Then
  \begin{equation}
    W_p(\mu_n,\mu) \to 0 \iff \int_X d(x,x_0)^p d\mu_n \to \int_X d(x,x_0)^p d\mu
    \text{ and } \mu_n \rightharpoonup \mu
  \end{equation}
  \label{thm:polishwmetrize}
\end{theorem}

Let's just put some words on these last two theorems we introduced.
We showed that the p-Wasserstein distance metrizes weak convergence
of probability measures in the space $\mathcal P_p(X)$, with $(X,d)$ a Polish space.
Such property is very useful and is not present in many other commonly used distances such as
Total Variation and the Kullback-Leibler Divergence.

Yet, there are many other ways to metrize weak convergence, such as Prokhorov's distance and bounded
Lipschitz distance. So, besides this \textit{metrization}, \citet{villani2008optimal}
gives the following reasons that make $W_p$ such an interesting metric:
\begin{enumerate}[(i)]
  \item It's definition makes it a natural choice in OT problems;
  \item The distance has a rich duality, especially for $p=1$;
  \item Since it's defined with an infimum, it is easy to bound from above;
  \item Wasserstein distances incorporate information of the ground geometry.
\end{enumerate}

For applications in Data Science, the equivalence with weak convergence and the
incorporation of the ground geometry are probably the most attractive characteristics.
Figure \ref{fig:wl-kl}
highlights how $W_p$ takes into account the underlying geometry compared
to the Kullback-Leibler divergence, which does not.


\begin{figure}[H]
  \centering
  \def\svgscale{0.7}
  \includesvg[inkscapelatex=false]{Figures/wassersteingeometry.svg}
	\caption{Comparison between Wasserstein distance and KL Divergence, based on \citet{montavon2016boltzmann}.
  On the left,
  there is a large overlap between the two distributions, but a large geometrical distance for a portion. On the right,
  there is much less overlap, but the whole distribution is geometrically closer. These two
  cases clearly highlight how $W_p$ incorporates geometrical information while $KL$ doesn't.}
	\label{fig:wl-kl}
\end{figure}

\newpage
\chapter{Optimal Transport Problems with Exact Solution}

In many cases, obtaining the exact solution to an OT problem might not be possible,
thus requiring the use of methods to approximate the real solution. Yet,
there are situations where it's possible to obtain the exact OT plan. This chapter
explores some of these situations.

\section{1D Optimal Transport}

\label{sec:1dOT}
For $\mu,\nu \in P_p(\mathbb R)$, the Wasserstein has a closed form solution, which
relies on the pseudoinverse of the cumulative distribution function.
\begin{definition}
  Let $\mu \in \mathcal P(\mathbb R)$. The cumulative distribution function (CDF) is
  \begin{equation}
    F_\mu(x) := \mu((-\infty,x])
  \end{equation}
  Note that $F_\mu$ is a nondecreasing and right-continuous function.
  \label{def:cumulativefunction}
\end{definition}

\begin{definition}
  Given a nondecreasing and right-continuous function $F:\mathbb R \to [0,1]$,
  its pseudoinverse is
  \begin{equation}
    F^{-1}(x):= \inf\{
    y \in \mathbb R \ : \ F(y) \geq x
    \}
  \end{equation}
  \label{def:pseudoinverse}
\end{definition}
After introducing these definitions, we can present the formula for computing the Wasserstein
distance (Remark 2.30 on \citet{peyre2019computational}):
\begin{equation}
  W_p(\mu,\nu)^p = \int_0^1| F_\mu^{-1}(x)- F_\nu^{-1}(x) |^p dx
\end{equation}
Note that for $p=1$ and $\mu,\nu$ both atomless, then
there exists an optimal Monge map $T = F_\nu^{-1} \circ F_\mu$.

For the discrete 1-D distributions, an even simpler algorithm can be devised. Let
$\mu = \sum^n_i=1 u_i \delta_{x_i}$ and
$\nu = \sum^m_j=1 v_i \delta_{v_j}$, where
$x_1\leq x_2 \leq ... \leq x_n$ and
$y_1\leq y_2 \leq ... \leq y_m$. Consider that each position $x_i$ has mass $u_i$
and each position $y_j$ has capacity $v_j$. The optimal transport plan
consists of moving particle $x_i$
to the closest position $y_j$, until capacity $v_j$ is filled.

\begin{figure}[H]
  \centering
  \def\svgscale{0.6}
  \includesvg[inkscapelatex=false]{Figures/ot-1d-discrete.svg}
  \caption{Illustration of the algorithm for optimally transporting distribution $\mu$ in blue
  to distribution $\nu$ in red.}
  \label{fig:ot-1d-discrete}
\end{figure}

\section{Transport Between Discrete Measures}

Let $\mu$ be a finite discrete probability measure, hence
\begin{equation}
  \mu := \sum^n_{i=1} u_i \delta_{x_i}
\end{equation}
Where $\mathbf x = (x_1,...,x_n) \in \mathbb R^{n\times d}$
represent the location of each mass particle $i \in \{1,...,n\}$. Vector
$\mathbf u \in \mathbb R^{n\times 1}$, with components $u_i \in (0,1]$,
is the vector of weights, representing the amount of ``mass'' of each particle. Hence,
discrete measures might be represented by a vector $\mathbf x$ of positions, and
$\mathbf u$ of weights.

Now, suppose that both $\mu$ and $\nu$ are discrete measures. Let $\mathbf u \in \mathbb R^{n\times 1}$
and $\mathbf v \in \mathbb R^{m \times 1}$ represent the vector of weights, and
$\mathbf x \in \mathbb R^{n\times d}, \mathbf y\in \mathbb R^{m\times d}$ represent the positions of each particle
of $\mu$ and $\nu$, respectively.
In this scenario, the Optimal Transport Problem might be reformulated as the following.
The cost function $c(x,y)$ can be substituted by a cost matrix $\mathbf C \in \mathbb R^{n \times m}$, where
\begin{equation}
  \mathbf C_{i,j} := c(x_i,y_j), \quad i \in \{1,...,n\}, \ j \in \{1,...,m\}
\end{equation}
Any transport plan $\gamma$ can be written as a matrix $\mathbf P \in \mathbb R_+^{n\times m}$, such that
$\mathbf P_{i,j}$ represents the amount of mass flowing from particle $i$ to particle $j$. Since
$\gamma \in \Pi(\mu,\nu)$, the set of possible transport plans can be written as:
\begin{equation}
  \mathbf U(\mathbf u, \mathbf v)
  := \left\{
  \mathbf P \in \mathbb R_+^{n\times m} \ : \ \mathbf P \mathbf 1_m = \mathbf u , \
  \mathbf P^\mathrm T \mathbf 1_n = \mathbf v
  \right\}
\end{equation}

Where $\bm 1_n$ is a vector with $n$ components, each equal to 1. In words, the sum
of each row of $\mathbf P$ must be equal to $\mathbf u$ and the sum of each column must
be equal to $\mathbf v$.

The Kantorovich Problem can be written as:
\begin{flalign}
  \text{(KP-Disc.)}&&
  \mathrm{L}_{\mathbf C}(\mathbf{u,v}) :=
  \min_{\mathbf P \in
    \mathbf U(\mathbf u, \mathbf v)
  } \langle \mathbf C, \mathbf P \rangle =
  \min_{\mathbf P \in
    \mathbf U(\mathbf u, \mathbf v)}
  \sum_{i=1}^n \sum_{j=1}^m \mathbf C_{i,j} \mathbf P_{i,j} &&
  \label{eq:kpdisc}
\end{flalign}

The Dual Problem becomes:
\begin{flalign}
  \text{(DP-Disc.)}&&
  \mathrm{L}_{\mathbf C}(\mathbf{u,v}) :=
  \max_{\mathbf (\mathbf f,\mathbf g) \in
    \mathbf R(\mathbf C)
  }
  \langle \mathbf f, \mathbf u \rangle
  +
  \langle \mathbf g, \mathbf v \rangle
  &&
\end{flalign}

Where
\begin{equation}
  \mathbf R(\mathbf C) :=
  \left\{
  (\mathbf f, \mathbf g) \in \mathbb R^n \times \mathbf R^m \ : \
  \forall (i,j) \in \{1,...,n\} \times \{1,...,m\}, \
  \mathbf f \oplus \mathbf g \leq \mathbf C
  \right\}
\end{equation}

The Discrete Optimal Transport Problem is actually a Linear Programming (LP) problem. Hence,
one can rearrange Equation \eqref{eq:kpdisc} to the standard form of LP.
\begin{definition}
  (Optimal Transport as standard LP)
  \begin{mini*}
    {}{\mathbf c^\mathrm T \mathbf p}{}{}
    \addConstraint{
      \mathbf{Ap} =
      \begin{bmatrix}
        \mathbf u \\
        \mathbf v
      \end{bmatrix}
    } {}{}{}
    \addConstraint{}{\mathbf p \geq 0}{}{}
  \end{mini*}
  Where
  \begin{align*}
    \mathbf p :=
    \begin{bmatrix}
      \mathbf P_{1,1} \\
      \mathbf P_{2,1} \\
      \vdots          \\
      \mathbf P_{n,1} \\
      \mathbf P_{2,1} \\
      \vdots          \\
      \mathbf P_{n,m}
    \end{bmatrix}
    , \quad
    \mathbf c :=
    \begin{bmatrix}
      \mathbf C_{1,1} \\
      \mathbf C_{2,1} \\
      \vdots          \\
      \mathbf C_{n,1} \\
      \mathbf C_{2,1} \\
      \vdots          \\
      \mathbf C_{n,m}
    \end{bmatrix}
    , \quad
    \mathbf A := \begin{bmatrix}
      \mathbf 1_n^\mathrm T \otimes \mathbf I_m \\
      \mathbf I_n               \otimes \mathbf 1_m^\mathrm T
    \end{bmatrix}, \\
  \end{align*}
  Note that $\mathbf I_n$ stands for the identity matrix, and $\otimes$ is the tensor product.
  \label{def:lpformat}
\end{definition}

\begin{definition}
  (Optimal Transport Dual Problem as LP)
  \begin{mini*}
    {}{
      \begin{bmatrix}
        \mathbf u \\
        \mathbf v
      \end{bmatrix}^\mathrm T \mathbf h}{}{}
    \addConstraint{\mathbf A^\mathrm T \mathbf h \leq \mathbf c} {}{}{}
  \end{mini*}
  Where $\mathbf h = [f_1,\ldots,f_n,g_1,\ldots,g_m]^\mathrm T$, with $\mathbf c$ and $\mathbf A$ the same as in the primal LP.
  \label{def:lpdual}
\end{definition}

Since the Optimal Transport Problem is actually a Linear Programming Problem, therefore, one can use known methods for solving such
problems, such as Simplex or Interior Point Method. An explanation on how to solve such LP problems in OT can be found in Chapter
3 of \citet{peyre2019computational}.

