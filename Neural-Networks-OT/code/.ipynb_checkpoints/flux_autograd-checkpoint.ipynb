{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980252af-1125-4ba1-8b08-9f18841b1d8d",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Neural Networks and Deep Learning</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Flux Introduction</h2>\n",
    "\n",
    "For the non-initiated, Flux.jl is currently (september/2021) the most starred package in the\n",
    "Julia ecossystem, and it's the go-to package in terms of Deep Neural Networks for Julia.\n",
    "\n",
    "\n",
    "Autograd: Automatic Differentiation\n",
    "===================================\n",
    "\n",
    "Automatic differentiation (the topic of this notebook) is \n",
    "Flux's core feature. It take a Julia function `f` and a set of arguments, returning the\n",
    "gradient with respect to each argument.\n",
    "\n",
    "\n",
    "*Ref: This tutorial is based on\n",
    "[this example tutorial from Flux's github page](https://github.com/FluxML/model-zoo/blob/master/tutorials/60-minute-blitz/60-minute-blitz.jl).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4600304-8c9c-460c-82d3-fc277938f0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/MEGA/EMAp/NeuralNetworks_Course/Notebooks/Flux/Project.toml`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.0,)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Pkg\n",
    "# This commands activates this folder environment.\n",
    "# It is similar to using a pyenv, but this is the way we do in Julia.\n",
    "# Note that we don't need external packages, Julia uses environments natively.\n",
    "# More about it can be found in the tutorial on Julia basics.\n",
    "Pkg.activate(\".\")\n",
    "\n",
    "# Let's import Flux as a package. This comes with the function `gradient` from Zygote.jl\n",
    "using Flux: gradient\n",
    "\n",
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "# Returns the gradient at 0.0.\n",
    "gradient(f,0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8fc8e-764c-43d0-a9a5-47c45cc04772",
   "metadata": {},
   "source": [
    "The `gradient` function uses automatic differentiation to calculate\n",
    "the derivative of polynomials.\n",
    "\n",
    "This does no work for any arbitrary\n",
    "function. Try for example, `f(x) = exp(x)` and you'll get an error.\n",
    "\n",
    "Below, we write another example for a function of three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6157f0b9-049a-4eba-a7fb-f3408a7720fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x,y,z) = y^3 + x^3 + x*y + z\n",
    "\n",
    "gradient(h,1,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b2d799-acc8-4a45-a9e5-682b232cb6d0",
   "metadata": {},
   "source": [
    "Now it's where things get interesting. We can take gradients\n",
    "of arrays.\n",
    "\n",
    "Take for example\n",
    "$Ax + b$, where $A$ is a 2 by 2 matrix, $x$ and $b$ are two dimensional vectors.\n",
    "This function actually returns another vector. So there is no gradient,\n",
    "but a jacobian. For this situation, we use the `jacobian` function from Zygote,\n",
    "which is not shipped with Flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbc514ca-93a6-4395-8a4c-82f3f9fc5b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0 0 0 0; 0 0 0 0], [1 2; 3 3], [1 0; 0 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Zygote: jacobian\n",
    "\n",
    "f(A,x,b) = A*x .+ b\n",
    "\n",
    "A = [1 2\n",
    "     3 3]\n",
    "b = [1,1]\n",
    "x = [0,0]\n",
    "\n",
    "jacobian(f,A,x,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4a29e3-ad73-40c7-8db2-9d16c9d62f7c",
   "metadata": {},
   "source": [
    "The reason that `jacobian` is not shipped on Flux is that Neural Networks\n",
    "usually only require the gradient in order to perform backward propagation. Hence,\n",
    "instead of $Ax + b$, we have functions such as $\\sum^n_{i=1} (Ax)_i + b_i$, which does\n",
    "have a gradient. Look the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a5e1ba1-dee1-4424-b1d5-34a05009b46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0 0.0; 0.0 0.0], [4.0, 5.0], [1.0, 1.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(A,x,b) = sum(A*x .+ b)\n",
    "\n",
    "A = [1 2\n",
    "     3 3]\n",
    "b = [1,1]\n",
    "x = [0,0]\n",
    "\n",
    "gradient(f,A,x,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b91d2-ab9c-4d21-ae9e-59e40f138ffc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><p>\n",
    "<strong>Obs</strong>: note the `Fill(1,2)` as the last element of the output of the `gradient` function.\n",
    "This is just a way to represent a vector of dimension 2 where all elements are equal to 1.\n",
    "if you want to underestand more about it, copy the code below and run it in a cell to see the output.\n",
    "\n",
    "```julia\n",
    "    \n",
    "    using FillArrays\n",
    "    @show collect(Fill(1,2))\n",
    "    @show collect(Fill(3.5,2,2))\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003463a3-fb95-472b-ba5c-bb06a568938d",
   "metadata": {},
   "source": [
    "It's even more impressive. It can take gradient of functions defined programmatically! \n",
    "Take a look at the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d83d215-d748-456f-8da0-ce2cc0ff9afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(gradient(mycrazyfunction, 0))[1] == exp(0) = true\n",
      "(gradient(mycrazyfunction, 1))[1] == cos(1) = true\n",
      "(gradient(mycrazyfunction, -10))[1] == 2 * -10 = true\n"
     ]
    }
   ],
   "source": [
    "function mycrazyfunction(x)\n",
    "    if x ≥ 1\n",
    "        return sin(x)\n",
    "    elseif -1 < x < 1\n",
    "        return exp(x)\n",
    "    else\n",
    "        return x^2\n",
    "    end\n",
    "end\n",
    "\n",
    "@show gradient(mycrazyfunction,0)[1] == exp(0)\n",
    "@show gradient(mycrazyfunction,1)[1] == cos(1)\n",
    "@show gradient(mycrazyfunction,-10)[1] == 2*-10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a62ba-b6fa-4a21-bfe0-6518d5c53942",
   "metadata": {},
   "source": [
    "Let's define a loss function similar to what we actually find in Neural Networks.\n",
    "Remember that the function `gradient()` will return a tuple with dimension `n`\n",
    "equal to the number of arguments that the loss function receives. In our example below,\n",
    "our `n=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc930f89-9eef-492b-9798-c741f08ddda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " -0.6711226626925774\n",
       "  1.0659156306091664\n",
       "  1.175536176922669\n",
       " -0.47252159253728543\n",
       "  1.5855710120528388"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myloss(W, b, x) = sum(W * x .+ b)\n",
    "\n",
    "W = randn(3, 5)\n",
    "b = zeros(3)\n",
    "x = [3,1,0,1,2]\n",
    "\n",
    "# Here we get the gradient in terms of the x at [3,1,0,1,2]\n",
    "gradient(myloss, W, b, x)[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad6991-1aa5-4744-a325-92d66ab6f6d1",
   "metadata": {},
   "source": [
    "When training a network, we might not need to, for example, take the gradient of `x`, since `x` represents our data.\n",
    "We only want to take the gradient of the parameters of the model, e.g. `W` and `b`.\n",
    "This can be done using the function `params` from Flux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7615a053-1819-4f79-8242-36d87ba18e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.0 1.0 … 1.0 2.0; 3.0 1.0 … 1.0 2.0; 3.0 1.0 … 1.0 2.0], Fill(1.0, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux: params\n",
    "\n",
    "\n",
    "W = randn(3, 5)\n",
    "b = zeros(3)\n",
    "x = [3,1,0,1,2]\n",
    "\n",
    "y(x) = sum(W * x .+ b)\n",
    "\n",
    "grads = gradient(()->y(x), params([W, b]))\n",
    "\n",
    "grads[W], grads[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "451519ee-936f-44c1-8290-86673da3f095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×5 Matrix{Float64}:\n",
       " -0.996638  -0.05127     -0.18591   -1.7349    -0.930081\n",
       " -1.15274    0.835228     0.960265  -0.874299   0.834282\n",
       "  0.596683  -0.00502874   1.90293   -0.86473    0.392212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad104d1-6744-48ce-bc6b-ab1cfbdf9ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8997f031-b148-4333-beb3-26544ea3020f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.959585942769246"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaFlux 1.6.3",
   "language": "julia",
   "name": "juliaflux-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
