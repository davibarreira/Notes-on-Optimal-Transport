{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60561d72-75a3-4a8a-810d-356ac8ce8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Iterators: partition\n",
    "using Flux\n",
    "using Flux.Optimise: update!\n",
    "using Flux: logitbinarycrossentropy\n",
    "using Images\n",
    "using MLDatasets\n",
    "using Statistics\n",
    "using Parameters: @with_kw\n",
    "using Random\n",
    "using Printf\n",
    "# using CUDAapi\n",
    "# using CuArrays\n",
    "using Zygote\n",
    "# if has_cuda()# Check if CUDA is available\n",
    "#     @info \"CUDA is on\"\n",
    "#     import CuArrays\t# If CUDA is available, import CuArrays\n",
    "#     CuArrays.allowscalar(false)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3a8035-130b-48fb-9f6d-177226efd7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@with_kw struct HyperParams\n",
    "    batch_size::Int = 128\n",
    "    latent_dim::Int = 100\n",
    "    epochs::Int = 30\n",
    "    n_critic::Int = 3\n",
    "    clip_value::Float32 = 0.01 \n",
    "    verbose_freq::Int = 1000\n",
    "    output_x::Int = 6        # No. of sample images to concatenate along x-axis \n",
    "    output_y::Int = 6        # No. of sample images to concatenate along y-axis\n",
    "    lr_dscr::Float64 = 0.00005\n",
    "    lr_gen::Float64 = 0.00005\n",
    "end\n",
    "\n",
    "## Generator and Discriminator\n",
    "function generator(args)\n",
    "    return Chain(Dense(args.latent_dim, 6272), x->leakyrelu.(x, 0.2f0), x-> reshape(x, 7,7,128, size(x,2)),\n",
    "            ConvTranspose((4, 4), 128 => 128; stride = 2, pad = 1),\n",
    "            BatchNorm(128, leakyrelu),\n",
    "            Dropout(0.25),\n",
    "            ConvTranspose((4, 4), 128 => 64; stride = 2, pad = 1),\n",
    "            BatchNorm(64, leakyrelu),\n",
    "            Conv((7, 7), 64 => 1, tanh; stride = 1, pad = 3)) |> gpu\n",
    "end\n",
    "\n",
    "function discriminator(args)\n",
    "    return Chain(Conv((3,3), 1=>128, pad=(1,1), stride = (2,2)),\n",
    "            x->leakyrelu.(x, 0.2f0),\n",
    "            Dropout(0.4),\n",
    "            Conv((3,3), 128=>128, pad= (1,1), stride = (2,2), leakyrelu),\n",
    "            x->leakyrelu.(x, 0.2f0),\n",
    "            x -> reshape(x, :, size(x, 4)),\n",
    "            Dropout(0.25),\n",
    "            Dense(6272, 1, sigmoid)) |> gpu\n",
    "end\n",
    "\n",
    "function load_data(hparams)\n",
    "    # Load MNIST dataset\n",
    "    images, labels = MLDatasets.MNIST.traindata(Float32)\n",
    "    # Normalize to [-1, 1] and convert it to WHCN\n",
    "    image_tensor = permutedims(reshape(@.(2f0 * images - 1f0), 28, 28, 1, :), (2, 1, 3, 4))\n",
    "    # Partition into batches\n",
    "    data = [image_tensor[:,:,:,r] |> gpu for r in partition(1:60000, hparams.batch_size)]\n",
    "    return data\n",
    "end\n",
    "\n",
    "function wasserstein_loss_discr(real, fake)\n",
    "    return -mean(real) + mean(fake)\n",
    "end\n",
    "\n",
    "function wasserstein_loss_gen(out)\n",
    "    return -mean(out)\n",
    "end\n",
    "\n",
    "function train_discr(discr, original_data, fake_data, opt_discr, hparams)\n",
    "    ps = Flux.params(discr)\n",
    "    loss = 0.0\n",
    "    for i in 1:hparams.n_critic\n",
    "        loss, back = Zygote.pullback(ps) do\n",
    "                        wasserstein_loss_discr(discr(original_data), discr(fake_data))\n",
    "        end\n",
    "        grads = back(1f0)\n",
    "        update!(opt_discr, ps, grads)\n",
    "        for i in ps\n",
    "            i = clamp.(i, -hparams.clip_value, hparams.clip_value)\n",
    "        end\n",
    "    end\n",
    "    return loss\n",
    "end\n",
    "\n",
    "Zygote.@nograd train_discr\n",
    "\n",
    "function train_gan(gen, discr, original_data, opt_gen, opt_discr, hparams)\n",
    "    noise = randn!(similar(original_data, (hparams.latent_dim, hparams.batch_size))) |> gpu\n",
    "    loss = Dict()\n",
    "    ps = Flux.params(gen)\n",
    "    loss[\"gen\"], back = Zygote.pullback(ps) do\n",
    "                          fake_ = gen(noise)\n",
    "                          loss[\"discr\"] = train_discr(discr, original_data, fake_, opt_discr, hparams)\n",
    "                          wasserstein_loss_gen(discr(fake_))\n",
    "    end\n",
    "    grads = back(1f0)\n",
    "    update!(opt_gen, ps, grads)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "function create_output_image(gen, fixed_noise, hparams)\n",
    "    @eval Flux.istraining() = false\n",
    "    fake_images = @. cpu(gen(fixed_noise))\n",
    "    @eval Flux.istraining() = true\n",
    "    image_array = dropdims(reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_y))); dims=(3, 4))\n",
    "    image_array = @. Gray(image_array + 1f0) / 2f0\n",
    "    return image_array\n",
    "end\n",
    "\n",
    "function train()\n",
    "    hparams = HyperParams()\n",
    "\n",
    "    data = load_data(hparams)\n",
    "\n",
    "    fixed_noise = [randn(hparams.latent_dim, 1) |> gpu for _=1:hparams.output_x*hparams.output_y]\n",
    "\n",
    "    # Discriminator\n",
    "    dscr = discriminator(hparams) \n",
    "\n",
    "    # Generator\n",
    "    gen =  generator(hparams) |> gpu\n",
    "\n",
    "    # Optimizers\n",
    "    opt_dscr = RMSProp(hparams.lr_dscr)\n",
    "    opt_gen = RMSProp(hparams.lr_gen)\n",
    "\n",
    "    isdir(\"output\")||mkdir(\"output\")\n",
    "\n",
    "    ones()\n",
    "    \n",
    "    # Training\n",
    "    train_steps = 0\n",
    "    for ep in 1:hparams.epochs\n",
    "        @info \"Epoch $ep\"\n",
    "        for x in data\n",
    "            # Update discriminator and generator\n",
    "            loss = train_gan(gen, dscr, x, opt_gen, opt_dscr, hparams)\n",
    "\n",
    "            if train_steps % hparams.verbose_freq == 0\n",
    "                @info(\"Train step $(train_steps), Discriminator loss = $(loss[\"discr\"]), Generator loss = $(loss[\"gen\"])\")\n",
    "                # Save generated fake image\n",
    "                output_image = create_output_image(gen, fixed_noise, hparams)\n",
    "                save(@sprintf(\"output/gan_steps_%06d.png\", train_steps), output_image)\n",
    "            end\n",
    "            train_steps += 1\n",
    "        end\n",
    "    end\n",
    "\n",
    "    output_image = create_output_image(gen, fixed_noise, hparams)\n",
    "    save(@sprintf(\"output/cgan_steps_%06d.png\", train_steps), output_image)\n",
    "end    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a495f1ee-0aca-4480-ba37-768920b55caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Train step 0, Discriminator loss = -0.051110864, Generator loss = -0.5066731\n",
      "└ @ Main In[10]:125\n",
      "┌ Info: Precompiling PNGFiles [f57f5aa1-a3ce-4bc8-8ab9-96f992907883]\n",
      "└ @ Base loading.jl:1342\n",
      "┌ Info: Epoch 2\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Epoch 3\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Train step 1000, Discriminator loss = 0.0, Generator loss = -1.0\n",
      "└ @ Main In[10]:125\n",
      "┌ Info: Epoch 4\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Epoch 5\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Train step 2000, Discriminator loss = 0.0, Generator loss = -1.0\n",
      "└ @ Main In[10]:125\n",
      "┌ Info: Epoch 6\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Epoch 7\n",
      "└ @ Main In[10]:119\n",
      "┌ Info: Train step 3000, Discriminator loss = 0.0, Generator loss = -1.0\n",
      "└ @ Main In[10]:125\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] process_events",
      "    @ ./libuv.jl:104 [inlined]",
      "  [2] wait()",
      "    @ Base ./task.jl:771",
      "  [3] yield()",
      "    @ Base ./task.jl:662",
      "  [4] synchronize(stream::CUDA.CuStream; blocking::Bool)",
      "    @ CUDA ~/.julia/packages/CUDA/9T5Sq/lib/cudadrv/stream.jl:128",
      "  [5] synchronize (repeats 2 times)",
      "    @ ~/.julia/packages/CUDA/9T5Sq/lib/cudadrv/stream.jl:117 [inlined]",
      "  [6] unsafe_copyto!(dest::Vector{Float32}, doffs::Int64, src::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, soffs::Int64, n::Int64)",
      "    @ CUDA ~/.julia/packages/CUDA/9T5Sq/src/array.jl:389",
      "  [7] copyto!",
      "    @ ~/.julia/packages/CUDA/9T5Sq/src/array.jl:349 [inlined]",
      "  [8] getindex",
      "    @ ~/.julia/packages/GPUArrays/0vqbc/src/host/indexing.jl:89 [inlined]",
      "  [9] #25",
      "    @ ~/.julia/packages/GPUArrays/0vqbc/src/host/indexing.jl:75 [inlined]",
      " [10] task_local_storage(body::GPUArrays.var\"#25#28\"{CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}}, key::Symbol, val::Bool)",
      "    @ Base ./task.jl:281",
      " [11] macro expansion",
      "    @ ~/.julia/packages/GPUArrays/0vqbc/src/host/indexing.jl:74 [inlined]",
      " [12] _mapreduce(f::typeof(identity), op::typeof(Base.add_sum), As::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}; dims::Colon, init::Nothing)",
      "    @ GPUArrays ~/.julia/packages/GPUArrays/0vqbc/src/host/mapreduce.jl:65",
      " [13] #mapreduce#20",
      "    @ ~/.julia/packages/GPUArrays/0vqbc/src/host/mapreduce.jl:28 [inlined]",
      " [14] mapreduce",
      "    @ ~/.julia/packages/GPUArrays/0vqbc/src/host/mapreduce.jl:28 [inlined]",
      " [15] #_sum#682",
      "    @ ./reducedim.jl:878 [inlined]",
      " [16] _sum",
      "    @ ./reducedim.jl:878 [inlined]",
      " [17] #sum#680",
      "    @ ./reducedim.jl:874 [inlined]",
      " [18] sum",
      "    @ ./reducedim.jl:874 [inlined]",
      " [19] _mean",
      "    @ ~/.julia/packages/GPUArrays/0vqbc/src/host/statistics.jl:17 [inlined]",
      " [20] #mean#2",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Statistics/src/Statistics.jl:164 [inlined]",
      " [21] #adjoint#670",
      "    @ ~/.julia/packages/Zygote/EPhp6/src/lib/array.jl:326 [inlined]",
      " [22] adjoint",
      "    @ ./none:0 [inlined]",
      " [23] _pullback",
      "    @ ~/.julia/packages/ZygoteRules/OjfTt/src/adjoint.jl:57 [inlined]",
      " [24] _pullback",
      "    @ ./In[10]:47 [inlined]",
      " [25] _pullback(::Zygote.Context, ::typeof(wasserstein_loss_discr), ::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, ::CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer})",
      "    @ Zygote ~/.julia/packages/Zygote/EPhp6/src/compiler/interface2.jl:0",
      " [26] _pullback",
      "    @ ./In[10]:59 [inlined]",
      " [27] _pullback(::Zygote.Context, ::var\"#16#17\"{Chain{Tuple{Conv{2, 2, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#8#11\", Dropout{Float64, Colon}, Conv{2, 2, typeof(leakyrelu), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#9#12\", var\"#10#13\", Dropout{Float64, Colon}, Dense{typeof(σ), CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}, CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}})",
      "    @ Zygote ~/.julia/packages/Zygote/EPhp6/src/compiler/interface2.jl:0",
      " [28] pullback",
      "    @ ~/.julia/packages/Zygote/EPhp6/src/compiler/interface.jl:352 [inlined]",
      " [29] train_discr(discr::Chain{Tuple{Conv{2, 2, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#8#11\", Dropout{Float64, Colon}, Conv{2, 2, typeof(leakyrelu), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#9#12\", var\"#10#13\", Dropout{Float64, Colon}, Dense{typeof(σ), CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}, original_data::CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, fake_data::CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, opt_discr::RMSProp, hparams::HyperParams)",
      "    @ Main ./In[10]:58",
      " [30] _pullback",
      "    @ ~/.julia/packages/Zygote/EPhp6/src/lib/grad.jl:8 [inlined]",
      " [31] _pullback",
      "    @ ./In[10]:78 [inlined]",
      " [32] _pullback(::Zygote.Context, ::var\"#20#21\"{Chain{Tuple{Dense{typeof(identity), CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#4#6\", var\"#5#7\", ConvTranspose{2, 4, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, BatchNorm{typeof(leakyrelu), CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Float32, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, Dropout{Float64, Colon}, ConvTranspose{2, 4, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, BatchNorm{typeof(leakyrelu), CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Float32, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, Conv{2, 4, typeof(tanh), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}, Chain{Tuple{Conv{2, 2, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#8#11\", Dropout{Float64, Colon}, Conv{2, 2, typeof(leakyrelu), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#9#12\", var\"#10#13\", Dropout{Float64, Colon}, Dense{typeof(σ), CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}, CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, RMSProp, HyperParams, Dict{Any, Any}, CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}})",
      "    @ Zygote ~/.julia/packages/Zygote/EPhp6/src/compiler/interface2.jl:0",
      " [33] pullback(f::Function, ps::Params)",
      "    @ Zygote ~/.julia/packages/Zygote/EPhp6/src/compiler/interface.jl:352",
      " [34] train_gan(gen::Chain{Tuple{Dense{typeof(identity), CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#4#6\", var\"#5#7\", ConvTranspose{2, 4, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, BatchNorm{typeof(leakyrelu), CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Float32, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, Dropout{Float64, Colon}, ConvTranspose{2, 4, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, BatchNorm{typeof(leakyrelu), CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}, Float32, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, Conv{2, 4, typeof(tanh), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}, discr::Chain{Tuple{Conv{2, 2, typeof(identity), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#8#11\", Dropout{Float64, Colon}, Conv{2, 2, typeof(leakyrelu), CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}, var\"#9#12\", var\"#10#13\", Dropout{Float64, Colon}, Dense{typeof(σ), CUDA.CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, CUDA.CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}}}}, original_data::CUDA.CuArray{Float32, 4, CUDA.Mem.DeviceBuffer}, opt_gen::RMSProp, opt_discr::RMSProp, hparams::HyperParams)",
      "    @ Main ./In[10]:76",
      " [35] train()",
      "    @ Main ./In[10]:122",
      " [36] top-level scope",
      "    @ In[11]:1",
      " [37] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [38] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaFlux 1.6.3",
   "language": "julia",
   "name": "juliaflux-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
